
.
===============================================================================
01. Introduction to Linux Storage Management 
===============================================================================


Linux storage management refers to the set of processes, tools, and techniques used to manage and control storage resources on Linux-based systems. 

                    This includes managing 
                    disk partitions, 
                    filesystems, 
                    logical volumes, 
                    RAID configurations, 
                    and other storage-related aspects. 

Effective storage management is crucial for optimizing performance, ensuring data integrity, and maximizing storage utilization. Here are key components of Linux storage management:


Disk Partitions:
---------------------------
          Partitioning Tools:
              Linux provides utilities like fdisk, parted, and gparted 
              for creating, modifying, and deleting disk partitions.
          
          
          Partition Types:
              Primary, extended, and logical partitions.


Filesystems:
---------------------------
            Filesystem Creation:
            Utilities like mkfs or mkfs.<filesystem> are used to 
            create filesystems on partitions.
            
            Filesystem Mounting:
            The mount command is used to attach a 
            filesystem to a specified mount point.
            
            Filesystem Types:
            Common Linux filesystems i
            nclude ext4, XFS, Btrfs, and others.
            



Logical Volume Management (LVM):
---------------------------
              LVM Components:
              Physical Volumes (PV), Volume Groups (VG), 
              and Logical Volumes (LV).
              
              
              Dynamic Storage Allocation:
              LVM allows for dynamic resizing of logical
              volumes and volume groups, providing 
              flexibility in storage management.



RAID (Redundant Array of Independent Disks):
------------------------------------------------------
              Software RAID:
              Linux supports software RAID 
              configurations using tools like mdadm. 
              RAID levels include 0 (striping), 1 (mirroring), 
              5 (striping with parity), and others.
              
              Hardware RAID:
              Some systems utilize hardware RAID controllers 
              for improved performance and reliability.



Swap Space Management:
---------------------------
    swapon / swapoff:
        The swapon and swapoff commands are used 
        to activate and deactivate swap space.
       


Monitoring and Reporting:
---------------------------
      df:
            Displays information about disk space usage on mounted filesystems.
      du:
            Estimates file space usage for directories.
      
      
      iostat, sar, vmstat:
            Monitoring tools for tracking system I/O, CPU, and memory statistics.
      



Disk Quotas:
---------------------------
    Quotacheck / Quotaon:
    Tools to enable and check disk quotas, 
    limiting user or group storage usage.




Backup and Restore:
---------------------------
      tar, rsync, dd:
          Tools for creating backups, 
          copying data, and performing disk cloning.
      
      Backup Strategies:
          Regular backups are crucial for data 
          protection and recovery in case of data loss.




Disk Encryption:
---------------------------
      dm-crypt / LUKS:
          Linux provides tools like dm-crypt and LUKS 
          for disk encryption to enhance data security.




Advanced Filesystems:
  ---------------------------      
        Btrfs, ZFS:
            Advanced filesystems with features such as 
            snapshotting, copy-on-write, and data integrity checks.



Linux storage management is essential for maintaining a 
reliable and efficient storage infrastructure.


Administrators should be familiar with these tools and concepts 
to optimize storage resources and ensure data integrity.








01_01-Welcome to Storage Management
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

                        ................
                        CENTOS7 Master .  =============> centos7 server1
                        ................  =============> centos7 server2


                    +-----------------------+    
                    |      Master Node      |
                    |  CentOS Machine 1     |
                    |                       |
                    +-----------------------+
                              ||
                              ||
              -----------------------------------
               |                               |
               v                               v
      +-----------------------+      +-----------------------+
      |      Server Node 1    |      |      Server Node 2    |
      |  CentOS Machine 2     |      |  CentOS Machine 3     |
      |                       |      |                       |
      +-----------------------+      +-----------------------+





[root@master1 ~]# hostname
master1
[root@master1 ~]# hostname -I
192.168.1.110


                  [root@server1 ~]# hostname
                  server1
                  [root@server1 ~]# hostname -I
                  192.168.1.111


                                    [root@server2 ~]# hostname
                                    server2
                                    [root@server2 ~]# hostname -I
                                    192.168.1.112



01_02-Listing Block Storage
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk     ------------------whole block disk size of 30G
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sr0                 11:0    1 1024M  0 rom    -----------------------DVD ROM






add disk on server1 == 8G from vm(VMWARE)
---------------------------------------
          stop server1 ==> settigs ==> Hard Disk ==> 
                                      add ==> SCSI 
                                          ==> create a virtual disk
                                          ==> max size of disk = 8G
                                          ==> disk file = lfcs2-server2.vmdk
          
              START MACHINE


[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk    ............. new disk added of 8GB
sr0                 11:0    1 1024M  0 rom




===============================================================================
02. Partitioning Disks
===============================================================================



02_01-Partitioning Linux Disks
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
Disk partitions are divisions or sections on a physical hard 
                disk drive (HDD) or 
                solid-state drive (SSD) 

that allow you to organize and manage your storage space. 

Partitioning is a crucial aspect of disk management in operating systems, including Linux. 



                  fdisk
                  gdisk
                  parted


---------------------------------------------------------
                    File systems
                    ext2, ext3, ext4,
                    reiserFS, XFS, btfrs
---------------------------------------------------------
        Logical      | Primary    | Partation
        Partations   | max 4      | max 128
---------------------------------------------------------
    MBR partitiin    | GUID Partition
    table of 2TB     | table 8 ZB
---------------------------------------------------------
                      Hard DISK
---------------------------------------------------------




Logical Separation:
----------------------------------
    Partitions logically divide a single physical disk into separate sections. 
    Each partition acts as an independent storage unit with its own filesystem.


Primary and Extended Partitions:
----------------------------------
    A hard disk can have up to four primary partitions or three primary 
    partitions and one extended partition.
    An extended partition can be further divided into multiple logical partitions.


Partition Table:
----------------------------------
    The Master Boot Record (MBR) or GUID Partition Table (GPT) contains 
    information about the disk's partition layout.
    MBR is commonly used for BIOS-based systems, while GPT is used for 
    UEFI-based systems and supports larger disks.
    

Partitioning Tools:
----------------------------------
fdisk: 
    A command-line tool for partitioning disks on Linux. 
    It's interactive and provides a text-based interface.
    
    fdisk /dev/sdX   # Open fdisk for /dev/sdX


parted: 
    A more modern, interactive, and feature-rich partitioning tool for Linux.
    
    parted /dev/sdX   # Open parted for /dev/sdX


gparted: 
    A graphical partition editor that simplifies partition management 
    with a user-friendly interface.
    
    
    gparted   # Launch the graphical partition editor






02_02-Using fdisk to Create Partitions
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk
sr0                 11:0    1 1024M  0 rom



[root@server1 ~]# fdisk -l /dev/sdb      ..................old command
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes







[root@server1 ~]# fdisk /dev/sdb .......... working on new disk 8G

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x54bcf669.

Command (m for help): m

Command (m for help): p

Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x54bcf669




Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)     ..... 4 primary partions MAX
   e   extended (container for logical partitions)
Select (default p):


Using default response p.
Partition number (1-4, default 1):
First sector (2048-16777215, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-16777215, default 16777215): +200M  ........... new 200M created

Created a new partition 1 of type 'Linux' and of size 200 MiB.


Command (m for help): p

Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x54bcf669

Device     Boot Start    End Sectors  Size Id Type
/dev/sdb1        2048 411647  409600  200M 83 Linux ........... 200 m of partions created






Command (m for help): n
Partition type
   p   primary (1 primary, 0 extended, 3 free)   ......... 4 of 1 primary used 
   e   extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (2-4, default 2): .............. default 2 priary is using
First sector (411648-16777215, default 411648):
Last sector, +sectors or +size{K,M,G,T,P} (411648-16777215, default 16777215): +300M ................. now again 300m of storate is been sliced

Created a new partition 2 of type 'Linux' and of size 300 MiB.

Command (m for help): p
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x54bcf669

Device     Boot  Start     End Sectors  Size Id Type
/dev/sdb1         2048  411647  409600  200M 83 Linux
/dev/sdb2       411648 1026047  614400  300M 83 Linux ........ new 300m size






Command (m for help): t
Partition number (1,2, default 2): 2
Hex code (type L to list all codes): 82

Changed type of partition 'Linux' to 'Linux swap / Solaris'.





Command (m for help): t
Partition number (1,2, default 2):
Hex code (type L to list all codes): L  .............. all file types

 0  Empty           24  NEC DOS         81  Minix / old Lin bf  Solaris
 1  FAT12           27  Hidden NTFS Win 


CTRL C or Z





[root@server1 ~]# fdisk -l /dev/sdb  ................... empty partion table
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes





root@server1 ~]# fdisk /dev/sdb
Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (1-4, default 1):
First sector (2048-16777215, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-16777215, default 16777215):

Created a new partition 1 of type 'Linux' and of size 8 GiB.

Command (m for help): p
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xd1c413e2

Device     Boot Start      End  Sectors Size Id Type
/dev/sdb1        2048 16777215 16775168   8G 83 Linux

Command (m for help): w    ........................ dow saving the changes
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.




[root@server1 ~]# fdisk -l /dev/sdb

Device     Boot Start      End  Sectors Size Id Type
/dev/sdb1        2048 16777215 16775168   8G 83 Linux



[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk
└─sdb1               8:17   0    8G  0 part    ........ new partion created of 8G
sr0                 11:0    1 1024M  0 rom






dd disk duplicator

[root@server1 ~]# dd if=/dev/zero of=/dev/sdb count=1 bs=512 ..........dd disk duplicator / wiping the disk partations
1+0 records in
1+0 records out
512 bytes copied, 0.000123773 s, 4.1 MB/s




[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk  ............. sdb1 is wiped out
sr0                 11:0    1 1024M  0 rom



[root@server1 ~]# fdisk -l /dev/sdb
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes



again sample example for 
crtearing 600m, 1200m,900m,1g size partions
------------------------------------------------------------

[root@server1 ~]# fdisk /dev/sdb

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (1-4, default 1):
First sector (2048-16777215, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-16777215, default 16777215): +600m

Created a new partition 1 of type 'Linux' and of size 600 MiB.




Command (m for help): n
Partition type
   p   primary (1 primary, 0 extended, 3 free)
   e   extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (2-4, default 2):
First sector (1230848-16777215, default 1230848):
Last sector, +sectors or +size{K,M,G,T,P} (1230848-16777215, default 16777215): +1200m

Created a new partition 2 of type 'Linux' and of size 1.2 GiB.






Command (m for help): n
Partition type
   p   primary (2 primary, 0 extended, 2 free)
   e   extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (3,4, default 3):
First sector (3688448-16777215, default 3688448):
Last sector, +sectors or +size{K,M,G,T,P} (3688448-16777215, default 16777215): +900m

Created a new partition 3 of type 'Linux' and of size 900 MiB.






Command (m for help): n
Partition type
   p   primary (3 primary, 0 extended, 1 free)
   e   extended (container for logical partitions)
Select (default e):

Using default response e.
Selected partition 4.
First sector (5531648-16777215, default 5531648):
Last sector, +sectors or +size{K,M,G,T,P} (5531648-16777215, default 16777215): +1G

Created a new partition 4 of type 'Extended' and of size 1 GiB.





Command (m for help): p

Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xaa1b3bd0

Device     Boot   Start     End Sectors  Size Id Type
/dev/sdb1          2048 1230847 1228800  600M 83 Linux  ...600m created
/dev/sdb2       1230848 3688447 2457600  1.2G 83 Linux  ....1200m created
/dev/sdb3       3688448 5531647 1843200  900M 83 Linux  .... 900m too
/dev/sdb4       5531648 7628799 2097152    1G  5 Extended  ....1g too




Command (m for help): w        ................saving the changes
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.






[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0  600M  0 part  ...... new +600m and like wise in sdb
├─sdb2               8:18   0  1.2G  0 part
├─sdb3               8:19   0  900M  0 part
└─sdb4               8:20   0    1K  0 part
sr0                 11:0    1 1024M  0 rom





[root@server1 ~]# fdisk -l /dev/sdb
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors

Device     Boot   Start     End Sectors  Size Id Type
/dev/sdb1          2048 1230847 1228800  600M 83 Linux
/dev/sdb2       1230848 3688447 2457600  1.2G 83 Linux
/dev/sdb3       3688448 5531647 1843200  900M 83 Linux
/dev/sdb4       5531648 7628799 2097152    1G  5 Extended




wiping disk data 
---------------

[root@server1 ~]# dd if=/dev/zero of=/dev/sdb count=1 bs=512
1+0 records in
1+0 records out
512 bytes copied, 0.000125273 s, 4.1 MB/s


[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk  .........................LOL
sr0                 11:0    1 1024M  0 rom










02_03-Using gdisk to Create Partitions and Restore Partitions
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



[root@server1 ~]# gdisk  /dev/sdb
GPT fdisk (gdisk) version 1.0.3

Warning: Partition table header claims that the size of partition table
entries is 0 bytes, but this program  supports only 128-byte entries.
Adjusting accordingly, but partition table may be garbage.
Warning: Partition table header claims that the size of partition table
entries is 0 bytes, but this program  supports only 128-byte entries.
Adjusting accordingly, but partition table may be garbage.
Partition table scan:
  MBR: not present
  BSD: not present
  APM: not present
  GPT: not present

Creating new GPT entries.

Command (? for help):





Command (? for help): n
Partition number (1-128, default 1): ....................... handle 128 parataions more than ...fdsik
First sector (34-16777182, default = 2048) or {+-}size{KMGTP}:
Last sector (2048-16777182, default = 16777182) or {+-}size{KMGTP}: +200M   .......... first 200m of partion creted
Current type is 'Linux filesystem'
Hex code or GUID (L to show codes, Enter = 8300):
Changed type of partition to 'Linux filesystem'

Command (? for help): p
Disk /dev/sdb: 16777216 sectors, 8.0 GiB
Model: VMware Virtual S
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 279930C8-5AEC-43E7-9A8E-360698FBA1E0
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 16777182
Partitions will be aligned on 2048-sector boundaries
Total free space is 16367549 sectors (7.8 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048          411647   200.0 MiB   8300  Linux filesystem







Command (? for help): n
Partition number (2-128, default 2):
First sector (34-16777182, default = 411648) or {+-}size{KMGTP}:
Last sector (411648-16777182, default = 16777182) or {+-}size{KMGTP}: +700M .... next 700m
Current type is 'Linux filesystem'
Hex code or GUID (L to show codes, Enter = 8300): 8200 ................. for swap partions
Changed type of partition to 'Linux swap'

Command (? for help): p
Disk /dev/sdb: 16777216 sectors, 8.0 GiB
Model: VMware Virtual S
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 279930C8-5AEC-43E7-9A8E-360698FBA1E0
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 16777182
Partitions will be aligned on 2048-sector boundaries
Total free space is 14933949 sectors (7.1 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048          411647   200.0 MiB   8300  Linux filesystem
   2          411648         1845247   700.0 MiB   8200  Linux swap  ...... 700m o swap partions





Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): Y
OK; writing new GUID partition table (GPT) to /dev/sdb.
The operation has completed successfully.




[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0  200M  0 part
└─sdb2               8:18   0  700M  0 part





[root@server1 ~]# gdisk -l /dev/sdb
Total free space is 14933949 sectors (7.1 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048          411647   200.0 MiB   8300  Linux filesystem
   2          411648         1845247   700.0 MiB   8200  Linux swap




wiping disk 
-----------
[root@server1 ~]# dd if=/dev/zero of=/dev/sdb count=2 bs=16K
2+0 records in
2+0 records out
32768 bytes (33 kB, 32 KiB) copied, 0.00016189 s, 202 MB/s




[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
sdb                  8:16   0    8G  0 disk
sr0                 11:0    1 1024M  0 rom




[root@server1 ~]# gdisk -l /dev/sdb


taking backup of last parted disk
--------------------------------------

[root@server1 ~]# gdisk -l /dev/sdb

Found invalid MBR and corrupt GPT. What do you want to do? (Using the
GPT MAY permit recovery of GPT data.)
 1 - Use current GPT
 2 - Create blank GPT

Your answer: 1
Disk /dev/sdb: 16777216 sectors, 8.0 GiB
Model: VMware Virtual S
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 279930C8-5AEC-43E7-9A8E-360698FBA1E0
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 16777182
Partitions will be aligned on 2048-sector boundaries
Total free space is 14933949 sectors (7.1 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048          411647   200.0 MiB   8300  Linux filesystem ............ backup restored of parted disk
   2          411648         1845247   700.0 MiB   8200  Linux swap








[root@server1 ~]# dd if=/dev/zero of=/dev/sdb  bs=16K






02_04-Using GNU Parted to Create Partitions
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# parted /dev/sdb print
Error: /dev/sdb: unrecognised disk label
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 8590MB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags:




[root@server1 ~]# parted /dev/sdb  .............. goint to disk sdb
GNU Parted 3.2
Using /dev/sdb
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) print  ................................. make print info
Error: /dev/sdb: unrecognised disk label
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 8590MB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags:
(parted)





(parted) mklabel msdos  .............. making label msdor aor any
(parted) print
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 8590MB
Sector size (logical/physical): 512B/512B
Partition Table: msdos  .................changes reflect here
Disk Flags:




(parted) mklabel gpt    .............. making GPT type
Warning: The existing disk label on /dev/sdb will be destroyed and all data on this
disk will be lost. Do you want to continue?
Yes/No? y        ....................... confirmation yes
(parted) print
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 8590MB
Sector size (logical/physical): 512B/512B
Partition Table: gpt    ................. changes of GPT reflect here




(parted) mklabel msdos
Yes/No? y
(parted) print
Partition Table: msdos



(parted) mkpart primary 1 200
(parted) print
Number  Start   End    Size   Type     File system  Flags
 1      1049kB  200MB  199MB  primary               lba




(parted) mkpart extended 201 -1    ................. 201m to entire last sector (-1)
(parted) print
Number  Start   End     Size    Type      File system  Flags
 1      1049kB  200MB   199MB   primary                lba
 2      201MB   8589MB  8388MB  extended               lba




(parted) mkpart logical 202 300
(parted) print
Partition Table: msdos
Number  Start   End     Size    Type      File system  Flags
 1      1049kB  200MB   199MB   primary                lba
 2      201MB   8589MB  8388MB  extended               lba
 5      202MB   300MB   97.5MB  logical                lba  ... logical goes number 5 coz ( partion number 1 2 3 4 are reserved for primary) and parted can do only MAX 15 partins , next HD ll be needed





(parted) quit
Information: You may need to update /etc/fstab.




[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0  190M  0 part  .... partion number goes from 16,17,18 ... check , goes upto 32 for isci
├─sdb2               8:18   0    1K  0 part
└─sdb5               8:21   0   93M  0 part
sr0                 11:0    1 1024M  0 rom



[root@server1 ~]# dd if=/dev/zero of=/dev/sdb count=1 bs=512
1+0 records in
1+0 records out


[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
sdb                  8:16   0    8G  0 disk
sr0                 11:0    1 1024M  0 rom









02_05-Scripting Partition Creation
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



[root@server1 ~]# cat partions_script.sh
#!/bin/bash
DISK="/dev/sdb"
echo " Create MBR partition table and extended partition across disk "
# Create MBR partition table and extended partition across disk
parted -s $DISK -- mklabel msdos mkpart extended 1m -1m

sleep 3

echo "Create swap partition as partition 5 the first logical parition "
#Create swap partition as partition 5 the first logical parition
parted -s $DISK mkpart logical linux-swap 2m 100m #5

sleep 3
parted -s $DISK mkpart logical 101m 200m     #6
parted -s $DISK mkpart logical 201m 300m     #7
parted -s $DISK mkpart logical 301m 400m     #8
parted -s $DISK mkpart logical 401m 500m     #9

sleep 3
echo "Create 3 more logical partitons for LVMs "
#Create 3 more logical partitons for LVMs
parted -s $DISK mkpart logical 501m 600m     #10
parted -s $DISK mkpart logical 601m 700m     #11
parted -s $DISK mkpart logical 701m 800m     #12


sleep 3
echo "set partion 10, 11, 12 to lvm"
parted -s $DISK set 10 lvm on   # set partion 10 to lvm
parted -s $DISK set 11 lvm on   # set partion 11 to lvm
parted -s $DISK set 12 lvm on   # set partion 12 to lvm

sleep 3

echo "Create 2 more partions for RAID  "
#Create 2 more partions for RAID
parted -s $DISK mkpart logical 801m 900m #13
parted -s $DISK mkpart logical 901m 1000m #14
parted -s $DISK set 13 raid on     # set partition 13 to RAID
parted -s $DISK set 14 raid on     # set partion 14 to RAID
parted -s $DISK print

echo "FINISHED "


[root@server1 ~]# chmod +x partions_script.sh


[root@server1 ~]# ./partions_script.sh
 Create MBR partition table and extended partition across disk
Create swap partition as partition 5 the first logical parition
Create 3 more logical partitons for LVMs
set partion 10, 11, 12 to lvm
Create 2 more partions for RAID

Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 8590MB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags:

Number  Start   End     Size    Type      File system  Flags
 1      1049kB  8589MB  8588MB  extended               lba
 5      2097kB  99.6MB  97.5MB  logical
 6      101MB   200MB   99.6MB  logical
 7      201MB   300MB   98.6MB  logical
 8      301MB   400MB   98.6MB  logical
 9      401MB   500MB   99.6MB  logical
10      501MB   600MB   98.6MB  logical                lvm
11      601MB   700MB   99.6MB  logical                lvm
12      701MB   800MB   98.6MB  logical                lvm
13      801MB   900MB   98.6MB  logical                raid
14      901MB   1000MB  99.6MB  logical                raid

FINISHED
[root@server1 ~]#


[root@server1 ~]# echo $?
0




[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0    1K  0 part
├─sdb5               8:21   0   93M  0 part
├─sdb6               8:22   0   95M  0 part
├─sdb7               8:23   0   94M  0 part
├─sdb8               8:24   0   94M  0 part
├─sdb9               8:25   0   95M  0 part
├─sdb10              8:26   0   94M  0 part
├─sdb11              8:27   0   95M  0 part
├─sdb12              8:28   0   94M  0 part
├─sdb13              8:29   0   94M  0 part
└─sdb14              8:30   0   95M  0 part
sr0                 11:0    1 1024M  0 rom







[root@server1 ~]# fdisk -l  /dev/sdb
Disk /dev/sdb: 8 GiB, 8589934592 bytes, 16777216 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x168e2837

Device     Boot   Start      End  Sectors Size Id Type
/dev/sdb1          2048 16775167 16773120   8G  f W95 Ext'd (LBA)
/dev/sdb5          4096   194559   190464  93M 82 Linux swap / Solaris
/dev/sdb6        196608   391167   194560  95M 83 Linux
/dev/sdb7        393216   585727   192512  94M 83 Linux
/dev/sdb8        587776   780287   192512  94M 83 Linux
/dev/sdb9        782336   976895   194560  95M 83 Linux
/dev/sdb10       978944  1171455   192512  94M 8e Linux LVM
/dev/sdb11      1173504  1368063   194560  95M 8e Linux LVM
/dev/sdb12      1370112  1562623   192512  94M 8e Linux LVM
/dev/sdb13      1564672  1757183   192512  94M fd Linux raid autodetect
/dev/sdb14      1759232  1953791   194560  95M fd Linux raid autodetect



dd if=/dev/sda  count=1 bs=512 of=/dev/sdb.mbr ....... backup of MBR

[root@server1 ~]#  dd if=/dev/sda  count=1 bs=512 of=/dev/sdb.mbr
1+0 records in
1+0 records out
512 bytes copied, 0.000129076 s, 4.0 MB/s




[root@server1 ~]# ls -lh /dev/sdb.mbr
-rw-r--r--. 1 root root 512 Feb 24 21:36 /dev/sdb.mbr






===============================================================================
03. Creating Linux File Systems
===============================================================================


03_01-General Purpose File Systems with EXT4
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
EXT4 is backward-compatible with its predecessors, EXT2 and EXT3. 

This means that existing EXT2 and EXT3 file systems can be
upgraded to EXT4 without reformatting.


EXT4 supports large file systems and file sizes, allowing for 
volumes up to 1 exabyte (1 EB) in size and individual
files up to 16 terabytes (16 TB).





sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0    1K  0 part
├─sdb5               8:21   0   93M  0 part
├─sdb6               8:22   0   95M  0 part
├─sdb7               8:23   0   94M  0 part
├─sdb8               8:24   0   94M  0 part
├─sdb9               8:25   0   95M  0 part
├─sdb10              8:26   0   94M  0 part
├─sdb11              8:27   0   95M  0 part
├─sdb12              8:28   0   94M  0 part
├─sdb13              8:29   0   94M  0 part
└─sdb14              8:30   0   95M  0 part
sr0                 11:0    1 1024M  0 rom



working in sdb6
------------------------

[root@server1 ~]# mkfs.ext4 -L DATA /dev/sdb6 .............. fromatting with ext4 in sdb6 of 95M
mke2fs 1.45.6 (20-Mar-2020)
Creating filesystem with 97280 1k blocks and 24384 inodes .......inodes
Filesystem UUID: adc89631-467c-415d-a4e7-a1becfd31337 ...... disk id
Superblock backups stored on blocks:
        8193, 24577, 40961, 57345, 73729

Allocating group tables: done
Writing inode tables: done
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done



[root@server1 ~]# tune2fs -L MYDATA -c 0 -i 0 /dev/sdb6
tune2fs 1.45.6 (20-Mar-2020)
Setting maximal mount count to -1
Setting interval between checks to 0 seconds










[root@server1 ~]# dumpe2fs /dev/sdb6        ....... taking info of metadata
dumpe2fs 1.45.6 (20-Mar-2020)
Filesystem volume name:   MYDATA    ..........volume label
Last mounted on:          <not available>
Filesystem UUID:          adc89631-467c-415d-a4e7-a1becfd31337
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file dir_nlink extra_isize metadata_csum
Filesystem flags:         signed_directory_hash
Default mount options:    user_xattr acl
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              24384    ............
Block count:              97280
Reserved block count:     4864          Block
Free blocks:              88549            info tabs
Free inodes:              24373 
First block:              1
Block size:               1024    .............
Fragment size:            1024
Group descriptor size:    64
Reserved GDT blocks:      256
Blocks per group:         8192
Fragments per group:      8192
Inodes per group:         2032
Inode blocks per group:   254
Flex block group size:    16
Filesystem created:       Sat Feb 24 21:40:42 2024
Last mount time:          n/a
Last write time:          Sat Feb 24 21:43:27 2024
Mount count:              0
Maximum mount count:      -1
Last checked:             Sat Feb 24 21:40:42 2024
Check interval:           0 (<none>)
Lifetime writes:          278 kB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:               128
Journal inode:            8
Default directory hash:   half_md4
Directory Hash Seed:      c3882acc-24ca-4390-b908-5ef332eaeac2
Journal backup:           inode blocks
Checksum type:            crc32c
Checksum:                 0xf6a118c3
Journal features:         (none)
Journal size:             4096k
Journal length:           4096
Journal sequence:         0x00000001
Journal start:            0


Group 0: (Blocks 1-8192) csum 0xff2f
  Primary superblock at 1, Group descriptors at 2-2
  Reserved GDT blocks at 3-258
  Block bitmap at 259 (+258), csum 0x0ebac842
  Inode bitmap at 271 (+270), csum 0x56af1615
  Inode table at 283-536 (+282)
  4848 free blocks, 2021 free inodes, 2 directories, 2021 unused inodes
  Free blocks: 3345-8192
  Free inodes: 12-2032
Group 1: (Blocks 8193-16384) csum 0xb6fb [INODE_UNINIT, BLOCK_UNINIT]
  Backup superblock at 8193, Group descriptors at 8194-8194
  Reserved GDT blocks at 8195-8450
  Block bitmap at 260 (bg #0 + 259), csum 0x00000000
  Inode bitmap at 272 (bg #0 + 271), csum 0x00000000
  Inode table at 537-790 (bg #0 + 536)
  7934 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 8451-16384
  Free inodes: 2033-4064
Group 2: (Blocks 16385-24576) csum 0xdecb [INODE_UNINIT, BLOCK_UNINIT]
  Block bitmap at 261 (bg #0 + 260), csum 0x00000000
  Inode bitmap at 273 (bg #0 + 272), csum 0x00000000
  Inode table at 791-1044 (bg #0 + 790)
  8192 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 16385-24576
  Free inodes: 4065-6096
Group 3: (Blocks 24577-32768) csum 0x2313 [INODE_UNINIT, BLOCK_UNINIT]
  Backup superblock at 24577, Group descriptors at 24578-24578
  Reserved GDT blocks at 24579-24834
  Block bitmap at 262 (bg #0 + 261), csum 0x00000000
  Inode bitmap at 274 (bg #0 + 273), csum 0x00000000
  Inode table at 1045-1298 (bg #0 + 1044)
  7934 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 24835-32768
  Free inodes: 6097-8128
Group 4: (Blocks 32769-40960) csum 0xd243 [INODE_UNINIT]
  Block bitmap at 263 (bg #0 + 262), csum 0x9751dff9
  Inode bitmap at 275 (bg #0 + 274), csum 0x00000000
  Inode table at 1299-1552 (bg #0 + 1298)
  4096 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 36865-40960
  Free inodes: 8129-10160
Group 5: (Blocks 40961-49152) csum 0x4817 [INODE_UNINIT, BLOCK_UNINIT]
  Backup superblock at 40961, Group descriptors at 40962-40962
  Reserved GDT blocks at 40963-41218
  Block bitmap at 264 (bg #0 + 263), csum 0x00000000
  Inode bitmap at 276 (bg #0 + 275), csum 0x00000000
  Inode table at 1553-1806 (bg #0 + 1552)
  7934 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 41219-49152
  Free inodes: 10161-12192
Group 6: (Blocks 49153-57344) csum 0x3ede [INODE_UNINIT, BLOCK_UNINIT]
  Block bitmap at 265 (bg #0 + 264), csum 0x00000000
  Inode bitmap at 277 (bg #0 + 276), csum 0x00000000
  Inode table at 1807-2060 (bg #0 + 1806)
  8192 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 49153-57344
  Free inodes: 12193-14224
Group 7: (Blocks 57345-65536) csum 0xebc0 [INODE_UNINIT, BLOCK_UNINIT]
  Backup superblock at 57345, Group descriptors at 57346-57346
  Reserved GDT blocks at 57347-57602
  Block bitmap at 266 (bg #0 + 265), csum 0x00000000
  Inode bitmap at 278 (bg #0 + 277), csum 0x00000000
  Inode table at 2061-2314 (bg #0 + 2060)
  7934 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 57603-65536
  Free inodes: 14225-16256
Group 8: (Blocks 65537-73728) csum 0x06c0 [INODE_UNINIT, BLOCK_UNINIT]
  Block bitmap at 267 (bg #0 + 266), csum 0x00000000
  Inode bitmap at 279 (bg #0 + 278), csum 0x00000000
  Inode table at 2315-2568 (bg #0 + 2314)
  8192 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 65537-73728 
  Free inodes: 16257-18288
Group 9: (Blocks 73729-81920) csum 0xfbd4 [INODE_UNINIT, BLOCK_UNINIT]
  Backup superblock at 73729, Group descriptors at 73730-73730
  Reserved GDT blocks at 73731-73986
  Block bitmap at 268 (bg #0 + 267), csum 0x00000000
  Inode bitmap at 280 (bg #0 + 279), csum 0x00000000
  Inode table at 2569-2822 (bg #0 + 2568)
  7934 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 73987-81920
  Free inodes: 18289-20320
Group 10: (Blocks 81921-90112) csum 0x93e4 [INODE_UNINIT, BLOCK_UNINIT]
  Block bitmap at 269 (bg #0 + 268), csum 0x00000000
  Inode bitmap at 281 (bg #0 + 280), csum 0x00000000
  Inode table at 2823-3076 (bg #0 + 2822)
  8192 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 81921-90112
  Free inodes: 20321-22352
Group 11: (Blocks 90113-97279) csum 0x749e [INODE_UNINIT]
  Block bitmap at 270 (bg #0 + 269), csum 0x46876e06
  Inode bitmap at 282 (bg #0 + 281), csum 0x00000000
  Inode table at 3077-3330 (bg #0 + 3076)
  7167 free blocks, 2032 free inodes, 0 directories, 2032 unused inodes
  Free blocks: 90113-97279
  Free inodes: 22353-24384








03_02-Enterprise Class File Systems with XFS
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0    1K  0 part
├─sdb5               8:21   0   93M  0 part
├─sdb6               8:22   0   95M  0 part
├─sdb7               8:23   0   94M  0 part
├─sdb8               8:24   0   94M  0 part
├─sdb9               8:25   0   95M  0 part
├─sdb10              8:26   0   94M  0 part
├─sdb11              8:27   0   95M  0 part
├─sdb12              8:28   0   94M  0 part
├─sdb13              8:29   0   94M  0 part
└─sdb14              8:30   0   95M  0 part
sr0                 11:0    1 1024M  0 rom






[root@server1 ~]# mkfs.xfs -b size=1k -l size=10m /dev/sdb7 
meta-data=/dev/sdb7              isize=512    agcount=4, agsize=24064 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=0 inobtcount=0
data     =                       bsize=1024   blocks=96256, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=1024   blocks=10240, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0




[root@server1 ~]# xfs_db -x /dev/sdb7        ...... viewing metadata
xfs_db> uuid
UUID = 84239d30-c957-4593-8b9f-07b94736f680
xfs_db>
xfs_db> label
label = ""
xfs_db> label DATA2
writing all SBs
new label = "DATA2"
xfs_db> label
label = "DATA2"
xfs_db> quit





03_03-Using the mount Command and EXT4 File Systems
+++++++++++++++++++++++++++++++++++++++++++++++++++++++ sdb6

[root@server1 ~]# mount /dev/sdb6 /mnt/

[root@server1 ~]# ls /mnt/
lost+found


[root@server1 ~]# umount /mnt

[root@server1 ~]# mount






spcifiic mount
-----------------------

[root@server1 ~]# mkdir -p /data/{mydata,mydata2}

[root@server1 ~]# ls -R /data/
/data/:
mydata  mydata2

/data/mydata:

/data/mydata2:








[root@server1 ~]# mount /dev/sdb6 /data/mydata



[root@server1 ~]# mount  | grep mydata
/dev/sdb6 on /data/mydata type ext4 (rw,relatime,seclabel)


[root@server1 ~]# mount -o remount,noexec /dev/sdb6 /data/mydata

[root@server1 ~]# mount  | grep mydata
/dev/sdb6 on /data/mydata type ext4 (rw,noexec,relatime,seclabel)



[root@server1 ~]# umount /data/mydata

[root@server1 ~]# mount  | grep mydata
[root@server1 ~]#

[root@server1 ~]# cat /proc/mounts



permenant mount
----------------------

[root@server1 ~]# blkid  /dev/sdb6
/dev/sdb6: LABEL="MYDATA" UUID="adc89631-467c-415d-a4e7-a1becfd31337" BLOCK_SIZE="1024" TYPE="ext4" PARTUUID="168e2837-06"


[root@server1 ~]# vi /etc/fstab
  UUID="adc89631-467c-415d-a4e7-a1becfd31337"             /data/mydata    ext4 noexec     0 2
~
:wq


[root@server1 ~]# mount -a


[root@server1 ~]# mount  | grep mydata
/dev/sdb6 on /data/mydata type ext4 (rw,noexec,relatime,seclabel)











03_04-Using the mount Command and XFS File Systems
+++++++++++++++++++++++++++++++++++++++++++++++++++++++ sdb7


[root@server1 mydata]# blkid  /dev/sdb7
/dev/sdb7: LABEL="DATA2" UUID="84239d30-c957-4593-8b9f-07b94736f680" BLOCK_SIZE="512" TYPE="xfs" PARTUUID="168e2837-07"




[root@server1 ~]# vi /etc/fstab
/dev/sdb7       /data/mydata2   xfs defaults    0 0



[root@server1 ~]# mount -a
[root@server1 ~]#



[root@server1 ~]# mount  | grep mydata2
/dev/sdb7 on /data/mydata2 type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)




[root@server1 ~]# cd /data/mydata2/
[root@server1 mydata2]# touch file1
[root@server1 mydata2]# ls file1
file1












[root@server1 mydata2]# xfs_info  /dev/sdb7  ............ checking metadata 
meta-data=/dev/sdb7              isize=512    agcount=4, agsize=24064 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=0 inobtcount=0
data     =                       bsize=1024   blocks=96256, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=1024   blocks=10240, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0












03_05-Mount Options
+++++++++++++++++++++++++++++++++++++++++++++++++++++++





[root@server1 ~]# man mount

    /INDEPENDENT

                      atime
                      noatime
                      
                      auto
                      noauto
                      
                      dev
                      nodev
                      
                      exec
                      noexec


                        diratime



      /FILESYSTEM-SPECIFIC MOUNT OPTIONS
      
      
      /ext4
      
      /xfs
















===============================================================================
04. Managing Swap and RAID Devices
===============================================================================

Swap space in Linux is a designated area on a storage 
device (usually a hard disk drive or a solid-state drive) that the
operating system uses as virtual memory. 

It serves as an extension of the system's physical memory (RAM) and 
provides additional space for storing data when the RAM is fully utilized. 

The primary purpose of swap space is to support the system's memory 
management and prevent the system from running out of memory.





Here are key aspects of swap space in Linux:
------------------------------------------------

Memory Extension:
--------------------------------
      When the physical RAM is exhausted and the system needs more 
      memory to store data, it transfers less frequently accessed or idle 
      portions of the memory to the swap space. 
      This allows the system to free up RAM for actively used processes.



Swap Devices:
--------------------------------
      Swap space can exist on dedicated partitions, swap files, or a 
      combination of both. Dedicated swap partitions are common, but swap 
      files provide more flexibility.



Swap Partition:
--------------------------------
      A dedicated partition on a storage device that is specifically 
      designated for use as swap space. It is created during the
      disk partitioning process.
      

Swap File:
--------------------------------
      A regular file on an existing filesystem that is used as swap space. 
      This provides a more flexible approach as it can be created, resized,
      or removed without modifying disk partitions.


Swapiness:
--------------------------------
      A kernel parameter that determines how aggressively the Linux
      kernel uses swap space. 
      
      The swappiness value ranges from 0 to 100, where lower 
      values (e.g., 10) result in less swapping and 
      higher values (e.g., 60) result in more swapping.


Emergency Storage:
--------------------------------
      Swap space acts as a safety net when the system is under heavy 
      load or when memory demands spike unexpectedly. 
      It allows the system to continue running by using disk space
      as a temporary memory extension.


Performance Impact:
--------------------------------
      While swap space prevents out-of-memory errors, excessive swapping 
      to disk can impact system performance because accessing data from
      disk is slower than accessing it from RAM.


Monitoring and Management:
--------------------------------
      System administrators can monitor swap space usage using tools 
      like free, top, or htop. 
      Proper management involves adjusting swappiness, adding or removing 
      swap space based on system requirements, and ensuring that swap
      space is properly configured to handle workload variations.


Hibernate and Suspend:
--------------------------------
      Swap space is often used for hibernation and suspend-to-disk 
      functionalities. 
      When a system is hibernated, the contents of the RAM are 
      saved to the swap space, allowing the system to resume its previous
      state upon reboot.



  To summarize, swap space in Linux is a crucial component of the
  virtual memory system, providing additional storage for data when
  the physical RAM is insufficient. 
  
  Properly configuring and managing swap space is important for
  ensuring optimal system performance and responsiveness.




04_01-Creating SWAP Space
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0    1K  0 part
├─sdb5               8:21   0   93M  0 part
├─sdb6               8:22   0   95M  0 part /data/mydata
├─sdb7               8:23   0   94M  0 part /data/mydata2
├─sdb8               8:24   0   94M  0 part

sr0                 11:0    1 1024M  0 rom



[root@server1 ~]# fdisk -l  /dev/sdb

Device     Boot   Start      End  Sectors Size Id Type
/dev/sdb1          2048 16775167 16773120   8G  f W95 Ext'd (LBA)
/dev/sdb5          4096   194559   190464  93M 82 Linux swap / Solaris ................ swap space created from script
/dev/sdb6        196608   391167   194560  95M 83 Linux
/dev/sdb7        393216   585727   192512  94M 83 Linux






[root@server1 ~]# mkswap /dev/sdb5
Setting up swapspace version 1, size = 93 MiB (97513472 bytes)
no label, UUID=29aa3148-d25b-4a9b-8610-0eb9b2da8596




[root@server1 ~]# swapon -s
Filename                                Type            Size    Used    Priority
/dev/dm-1                               partition       3145724 0       -2    ...................... low priority




[root@server1 ~]# swapon /dev/sdb5
[root@server1 ~]# swapon -s
Filename                                Type            Size    Used    Priority
/dev/dm-1                               partition       3145724 0       -2  ................... priority => -1 HIGH , -5 LOW
/dev/sdb5                               partition       95228   0       -3
[root@server1 ~]#






[root@server1 ~]# swapoff /dev/sdb5

[root@server1 ~]# swapon -s
Filename                                Type            Size    Used    Priority
/dev/dm-1 






[root@server1 ~]# swapon /dev/sdb5

[root@server1 ~]# swapon -s
Filename                                Type            Size    Used    Priority
/dev/dm-1                               partition       3145724 0       -2
/dev/sdb5                               partition       95228   0       -3

[root@server1 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           1612         246        1095           8         269        1207
Swap:          3164           0        3164
[root@server1 ~]#






04_02-Setting the SWAP Priority
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# swapoff  -a


[root@server1 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           1612         244        1098           8         269        1209
Swap:             0           0           0


[root@server1 ~]# swapon  -s


[root@server1 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           1612         246        1095           8         269        1207
Swap:          3071           0        3071



permenant persistance of swap
--------------------------------

[root@server1 ~]# vi /etc/fstab

:r!blkid /dev/sdb5    ................. running command inside vim

UUID="29aa3148-d25b-4a9b-8610-0eb9b2da8596"     swap    swap    sw,pri=5        0 0


                :wq




[root@server1 ~]# swapon -a        .................. a for activation
[root@server1 ~]# swapon -s        ................... s for summary
Filename                                Type            Size    Used    Priority
/dev/dm-1                               partition       3145724 0       -2
/dev/sdb5                               partition       95228   0       5    .......... priority on 5







04_03-Configuring Software RAID
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


        Software RAID (Redundant Array of Independent Disks) is a 
        storage technology that allows multiple physical disk drives 
        to be combined into a single logical unit for the purposes of 
        data redundancy, performance improvement, or both.


Levels of Software RAID:
-------------------------
          Software RAID supports different RAID levels, including 
                    RAID 0 (striping), 
                    RAID 1 (mirroring), 
                    RAID 5 (striping with parity), 
                    RAID 6 (striping with double parity), 
          
          and more. 
          
          The specific RAID level chosen depends on the desired balance
          between performance and data redundancy.



        On Linux systems, the 
                  mdadm (Multiple Device Admin) 
        utility is commonly used to 
        create, manage, and monitor software RAID configurations



Before RAID Configuration:
    Two separate disks (Disk A and Disk B).

                        Disk A         Disk B
                        |             |
                        |             |
                        |             |


Create RAID 1 Configuration:
    RAID 1 involves mirroring, so data is 
    duplicated on both disks for redundancy.

                        Disk A         Disk B
                        |             |
                        |    RAID 1   |
                        |    Mirror   |
                        |             |





RAID 1 Operation:
    In RAID 1, if data is written to Disk A, it 
    is also simultaneously written to Disk B
    to ensure redundancy.

                      Disk A         Disk B
                      |   Write     |   Write
                      |    RAID 1   |    RAID 1
                      |    Mirror   |    Mirror
                      |             |




[root@server1 ~]# fdisk -l /dev/sdb

/dev/sdb13      1564672  1757183   192512  94M fd Linux raid autodetect
/dev/sdb14      1759232  1953791   194560  95M fd Linux raid autodetect

                        /dev/sdb13      ......... on same device /dev/sdb partions is not good 
                        /dev/sdb14       ......... problem of fauld tolerence, add extra physical disk to it
  



[root@server1 ~]# cat /proc/mdstat    ......... no raid devices configured
Personalities :
unused devices: <none>


conf of RAID-1 in sdb13 and 14, with 2 devices
--------------------------------------------

[root@server1 ~]# mdadm --create --verbose /dev/md0 --level=mirror --raid-devices=2 /dev/sdb13 /dev/sdb14      
mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: size set to 95232K
mdadm: largest drive (/dev/sdb14) exceeds size (95232K) by more than 1%
Continue creating array? y    ............................................................... give yes confirmations
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.










[root@server1 ~]# ls -l /dev/md0
brw-rw----. 1 root disk 9, 0 Feb 25 06:35 /dev/md0    .............. blovk device created



[root@server1 ~]# cat /proc/mdstat
Personalities : [raid1]
md0 : active raid1 sdb14[1] sdb13[0]  ....................... raid1 created with sdb13 n 14
      95232 blocks super 1.2 [2/2] [UU]

unused devices: <none>






[root@server1 ~]# lsmod  | grep raid
raid1                  53248  1




[root@server1 ~]# mkfs.xfs /dev/md0  ................ creating block device md0 of raid1 in xfs file system




[root@server1 ~]# mdadm --detail --scan    ...... listing the raid setup or array is setup 
ARRAY /dev/md0 metadata=1.2 name=server1:0 UUID=0f32750b:9a135de3:df63ebf3:ff9657d4




[root@server1 ~]# mdadm --detail --scan >> /etc/mdadm.conf    ...... send conf file in /etc of RAID



[root@server1 ~]# mdadm --stop /dev/md0      ............ sttoping raid md0 block storage
mdadm: stopped /dev/md0



[root@server1 ~]# mdadm --assemble --scan













===============================================================================
05. Extending Permissions with ACLS
===============================================================================

Access Control Lists (ACLs) are used to extend or enhance 
the traditional Unix file permissions system. 

While traditional Unix permissions (read, write, 
execute for owner, group, and others) are straightforward,
ACLs provide a more fine-grained control over file and directory 
access by allowing additional rules for specific users or groups.


Basic ACL Concepts:
-------------------------------

Default Unix Permissions:
-------------------------------
        Traditional Unix permissions consist of 
            read, 
            write, and 
            execute 
        
        permissions for the owner, group, and others.


ACL Entries:
-------------------------------
        ACLs introduce additional entries for specific 
            users or groups, 
        allowing you to set more granular permissions.




Setting ACLs:
-------------------------------
      You can set ACLs using commands like 
          setfacl 
      on Linux systems.




ACL Entries Syntax:
------------------------
u: 
    followed by the username 
    specifies ACL for a specific user.

g: 
    followed by the group name 
    specifies ACL for a specific group.

mask: 
    specifies the maximum permissions 
    that can be granted by the ACL entries.

other: 
    specifies the default permissions 
    for users not covered by other ACL entries.





Important Notes:
-----------------------
      * ACLs are supported on many Unix-like systems, including Linux.
      * Be cautious when using ACLs to avoid overly complex permission structures.
      * Regular Unix permissions still apply in conjunction with ACLs.
      * ACLs can be inherited by child directories and files.

    Always refer to the man pages (man setfacl) or documentation
    specific to your operating system for detailed and accurate
    information on ACL usage on your system.









05_01-Introducing ACLs in Linux
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



05_02-ACL Support within the Kernel and Filesystems
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



[root@server1 ~]# useradd mivaan ............... create a noral user
[root@server1 ~]# passwd mivaan

[root@server1 ~]# su - mivaan


[mivaan@server1 ~]$ whoami
mivaan



[mivaan@server1 ~]$ uname -r    .......................... viewing kernel of ur OS
4.18.0-540.el8.x86_64


[mivaan@server1 ~]$ ls /boot/config-4.18.0-540.el8.x86_64
/boot/config-4.18.0-540.el8.x86_64



[mivaan@server1 ~]$ grep ACL /boot/config-$(uname -r)
CONFIG_EXT4_FS_POSIX_ACL=y
CONFIG_XFS_POSIX_ACL=y
CONFIG_FS_POSIX_ACL=y
CONFIG_TMPFS_POSIX_ACL=y
CONFIG_NFS_V3_ACL=y
CONFIG_NFSD_V2_ACL=y
CONFIG_NFSD_V3_ACL=y
CONFIG_NFS_ACL_SUPPORT=m
CONFIG_CEPH_FS_POSIX_ACL=y
 



[mivaan@server1 ~]$ mount | grep data
/dev/sdb6 on /data/mydata type ext4 (rw,noexec,relatime,seclabel)
/dev/sdb7 on /data/mydata2 type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)




[mivaan@server1 ~]$ sudo tune2fs -l /dev/sdb6  | grep -i default
Default mount options:    user_xattr acl    ............................. user_extended and acl attributes for mount options
Default directory hash:   half_md4








05_03-Listing Filesystem ACLs
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


[mivaan@server1 ~]$ pwd
/home/mivaan


[mivaan@server1 ~]$ ls -A
.bash_logout  .bash_profile  .bashrc


[mivaan@server1 ~]$ ls -a
.  ..  .bash_logout  .bash_profile  .bashrc




[mivaan@server1 ~]$ ls -l
total 0
c 1 root   root   0 Feb 25 08:01 file1 
-rw-r--r--. 1 root   root   0 Feb 25 08:01 file2
-rw-rw-r--. 1 mivaan mivaan 0 Feb 25 08:01 myfile4
-rw-rw-r--. 1 mivaan mivaan 0 Feb 25 08:01 myflr3



                    -rw-r--r--.    -------------(.) dot , is entry reprenstnt ACL




[mivaan@server1 ~]$ getfacl  file1    .................. ACL of any file
# file: file1
# owner: root
# group: root
user::rw-
group::r--
other::r--


[mivaan@server1 ~]$ getfacl  myfile4
# file: myfile4
# owner: mivaan
# group: mivaan
user::rw-
group::rw-
other::r--







05_04-Setting Default ACLS
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
[mivaan@server1 ~]$ mkdir test-acl

[mivaan@server1 ~]$ ls -ld test-acl/
drwxrwxr-x. 2 mivaan mivaan 6 Feb 25 08:05 test-acl/    ............ (.) dot says no ACL is configured , it means a ACL support


        

[mivaan@server1 ~]$ getfacl  test-acl/
# file: test-acl/
# owner: mivaan
# group: mivaan
user::rwx
group::rwx
other::r-x                ........... other can read  mivaan file by defauld, due to umask



[mivaan@server1 ~]$ umask        ............deafut permission made in OS
0002


setting acl on dir, -m: modify, d: default, o: others , ---: nothing permission
------------------------------------------------------------------------------

[mivaan@server1 ~]$ setfacl -m d:o:--- test-acl/  



[mivaan@server1 ~]$ ls -ld test-acl/
drwxrwxr-x+ 2 mivaan mivaan 6 Feb 25 08:05 test-acl/    ..........(+) ACL is setup



[mivaan@server1 ~]$ getfacl  test-acl/
# file: test-acl/
# owner: mivaan
# group: mivaan
user::rwx
group::rwx
other::r-x
default:user::rwx
default:group::rwx
default:other::---        .................. checkng WHAT ACL is etup as , others as nothing , i.e ---










test on file
--------------


[mivaan@server1 ~]$ touch test-acl/file1

[mivaan@server1 ~]$ ls -l test-acl/file1
-rw-rw----. 1 mivaan mivaan 0 Feb 25 08:15 test-acl/file1      .......... ACL (.) not set



[mivaan@server1 ~]$ getfacl test-acl/file1
# file: test-acl/file1
# owner: mivaan
# group: mivaan
user::rw-
group::rw-
other::---




[mivaan@server1 ~]$ setfacl -d -m u:ram:rw test-acl/


[mivaan@server1 ~]$ getfacl test-acl/
# file: test-acl/
# owner: mivaan
# group: mivaan
user::rwx
group::rwx
other::r-x
default:user::rwx
default:user:ram:rw-        ............ACL given for ram
default:group::rwx
default:mask::rwx
default:other::---




[mivaan@server1 ~]$ touch test-acl/file2

[mivaan@server1 ~]$ ls -l test-acl/
total 0
-rw-rw----. 1 mivaan mivaan 0 Feb 25 08:15 file1
-rw-rw----+ 1 mivaan mivaan 0 Feb 25 08:21 file2      ...............ACL is set for file2 ....


[mivaan@server1 ~]$ getfacl test-acl/file1
# file: test-acl/file1
# owner: mivaan
# group: mivaan
user::rw-
group::rw-
other::---



[mivaan@server1 ~]$ getfacl test-acl/file2
# file: test-acl/file2
# owner: mivaan
# group: mivaan
user::rw-
user:ram:rw-            ..................................ACL for ram after setfacl
group::rwx                      #effective:rw-
mask::rw-
other::---


















05_05-Adding ACL Entries
+++++++++++++++++++++++++++++++++++++++++++++++++++++++





[mivaan@server1 ~]$ su root
Password:


[root@server1 ~]#


[root@server1 ~]# mkdir /work


[root@server1 ~]# ls -ld /work/
drwxr-xr-x. 2 root root 6 Feb 25 08:26 /work/


[root@server1 ~]# chmod  o= /work/


[root@server1 ~]# su  - mivaan
[mivaan@server1 ~]$ cd /work/
-bash: cd: /work/: Permission denied    .......... not permission for mivaan


[mivaan@server1 ~]$ ls -ld /work/
drwxr-x---. 2 root root 6 Feb 25 08:26 /work/










[mivaan@server1 ~]$ exit
logout
[root@server1 ~]#
[root@server1 ~]# setfacl -m u:mivaan:rx /work/

[root@server1 ~]# getfacl /work/
getfacl: Removing leading '/' from absolute path names
# file: work/
# owner: root
# group: root
user::rwx
user:mivaan:r-x      .............set for mivaan :rx
group::r-x
mask::r-x
other::---



[root@server1 ~]# su - mivaan
[mivaan@server1 ~]$ cd /work/    ................. now mivaan has Access to the /wok folder using ACL
[mivaan@server1 work]$ touch file3
touch: cannot touch 'file3': Permission denied









 




05_06-Removing ACLS
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




05_07-Diagnosing and Resolving Security Issues
+++++++++++++++++++++++++++++++++++++++++++++++++++++++











===============================================================================
06. Managing Logical Volumes
===============================================================================

Logical Volumes (LVs) are a concept within the Logical Volume Manager (LVM) 
system used in Linux. 

LVM is a flexible and dynamic storage management system that allows for
easy management of disk space, resizing of filesystems, and other
advanced storage features.


Basic Concepts:
------------------------------

    Physical Volumes (PVs):
    ------------------------------
            Physical Volumes are physical storage devices like hard drives 
            or partitions that are part of the LVM system.
    
    Volume Groups (VGs):
    ------------------------------
            Volume Groups are logical containers that consist of one or
            more Physical Volumes.
    
    
    Logical Volumes (LVs):
    ------------------------------
          Logical Volumes are slices or partitions within Volume Groups. 
          They are the storage entities that are actually used 
          by the operating system.



06_01-Welcome to LVM2
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

                          +--------------------------+
                          |   Create Physical Volume |
                          |                          |
                          |   pvcreate /dev/sdb      |
                          +--------------------------+
                                        |
                                        v
                          +--------------------------+
                          |   Create Volume Group    |
                          |                          |
                          |   vgcreate myvg /dev/sdb |
                          +--------------------------+
                                        |
                                        v
                          +--------------------------+
                          |   Create Logical Volume  |
                          |                          |
                          | lvcreate -L 20G -n mylv  |
                          | myvg                     |
                          +--------------------------+





    ******************************************************


+-----------------------+
|                       |
| Create Physical Volume|
|   pvcreate /dev/sdb   |
|                       |
+-----------------------+       +-------------------------+
                                |                         |
+-----------------------+       | Create Volume Group     |
|                       |       |   vgcreate myvg /dev/sdb|
| Create Volume Group   |       |                         |
| vgcreate myvg /dev/sdb|       +-------------------------+
|                       |
+-----------------------+       +-------------------------+
                                |                         |
+-----------------------+       | Create Logical Volume   |
|                       |       | lvcreate -L 20G -n mylv |
| Create Logical Volume |       | myvg                    |
|lvcreate -L 20G -n mylv|      |                         |
| myvg                  |       +-------------------------+
+-----------------------+








06_02-Creating LVMs in CentOS 7
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


[root@server1 ~]# pvscan
  PV /dev/sda2   VG cs_master       lvm2 [<29.00 GiB / 0    free]
  Total: 1 [<29.00 GiB] / in use: 1 [<29.00 GiB] / in no VG: 0 [0   ]





[root@server1 ~]# vgscan
  Found volume group "cs_master" using metadata type lvm2





[root@server1 ~]# lvscan
  ACTIVE            '/dev/cs_master/swap' [3.00 GiB] inherit
  ACTIVE            '/dev/cs_master/root' [<26.00 GiB] inherit








[root@server1 ~]# fdisk -l /dev//sdb

Device      Boot   Start      End  Sectors Size Id Type
/dev//sdb10       978944  1171455   192512  94M 8e Linux LVM  .................... chcking the partioned disk
/dev//sdb11      1173504  1368063   194560  95M 8e Linux LVM
/dev//sdb12      1370112  1562623   192512  94M 8e Linux LVM




                        +---------------------------------------+
                        |                                       |
                        |   Create PVs                          |
                        |   pvcreate /dev/sdb10                 |
                        |   pvcreate /dev/sdb11                 |
                        |   pvcreate /dev/sdb12                 |
                        |                                       |
                        +---------------------------------------+  
                              |                              |
                              |                              V
                              |                       +-----------------------+
                              V                      |                       |
                  +-----------------------+         |   Create VG           |
                  |                       |         |   vgcreate vg1        |
                  |   Create VG           |         |   /dev/sdb10          |
                  |   vgcreate vg1        |         |   /dev/sdb11          |
                  |   /dev/sdb10          |         |                       |
                  |   /dev/sdb11          |         +-----------------------+
                  |   /dev/sdb12          |                     |
                  +-----------------------+                     v
                                                     +-----------------------+
                                                    |                       |
                    +-----------------------+       |   Create LV           |
                    |                       |       |   lvcreate -n lv1      |
                    |   Create LV           |       |   -L 184M             |
                    |   lvcreate -n lv1     |       |   vg1                 |
                    |   -L 184M             |       |                       |
                    |   vg1                 |       +-----------------------+
                    +-----------------------+




creating physical volume on sdb 10 11 12
-------------------------------------------
[root@server1 ~]# pvcreate /dev/sdb10
  Physical volume "/dev/sdb10" successfully created.
[root@server1 ~]# pvcreate /dev/sdb11
  Physical volume "/dev/sdb11" successfully created.
[root@server1 ~]# pvcreate /dev/sdb12
  Physical volume "/dev/sdb12" successfully created.





volume grpup on sdb10 and sdb11
----------------------------------------

[root@server1 ~]# vgcreate vg1 /dev/sdb10 /dev/sdb11
  Volume group "vg1" successfully created





[root@server1 ~]# vgscan
  Found volume group "vg1" using metadata type lvm2
  Found volume group "cs_master" using metadata type lvm2



[root@server1 ~]# vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  cs_master   1   2   0 wz--n- <29.00g      0
  vg1         2   0   0 wz--n- 184.00m 184.00m



logical volume created
--------------------------
[root@server1 ~]# lvcreate -n lv1 -L 184m vg1
  Logical volume "lv1" created.





[root@server1 ~]# lvscan
  ACTIVE            '/dev/vg1/lv1' [184.00 MiB] inherit
  ACTIVE            '/dev/cs_master/swap' [3.00 GiB] inherit
  ACTIVE            '/dev/cs_master/root' [<26.00 GiB] inherit




[root@server1 ~]# lvs
  LV   VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root cs_master -wi-ao---- <26.00g                                
  swap cs_master -wi-ao----   3.00g                                
  lv1  vg1       -wi-a----- 184.00m 




[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part  /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm   /
  └─cs_master-swap 253:1    0    3G  0 lvm   [SWAP]
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0    1K  0 part
├─sdb5               8:21   0   93M  0 part  [SWAP]
├─sdb6               8:22   0   95M  0 part  /data/mydata
├─sdb7               8:23   0   94M  0 part  /data/mydata2
├─sdb8               8:24   0   94M  0 part
├─sdb9               8:25   0   95M  0 part
├─sdb10              8:26   0   94M  0 part
│ └─vg1-lv1        253:2    0  184M  0 lvm
├─sdb11              8:27   0   95M  0 part
│ └─vg1-lv1        253:2    0  184M  0 lvm
├─sdb12              8:28   0   94M  0 part
├─sdb13              8:29   0   94M  0 part
│ └─md0              9:0    0   93M  0 raid1
└─sdb14              8:30   0   95M  0 part
  └─md0              9:0    0   93M  0 raid1
sr0                 11:0    1 1024M  0 rom




making fie system for that lvm i.e lv1
-------------------------------------

[root@server1 ~]# mkfs.xfs /dev/vg1/lv1



mount it
-----------
[root@server1 ~]# mkdir /my-lvm


[root@server1 ~]# vi /etc/fstab

/dev/vg1/lv1    /my-lvm         xfs     defaults        0 0


[root@server1 ~]# mount -a



view mount
-------------

[root@server1 ~]# df -h

/dev/sda1                  1014M  300M  715M  30% /boot
/dev/sdb6                    87M   15K   80M   1% /data/mydata
/dev/sdb7                    84M  7.1M   77M   9% /data/mydata2
tmpfs                       162M     0  162M   0% /run/user/0
/dev/mapper/vg1-lv1         179M   11M  168M   6% /my-lvm






[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT

├─sdb10              8:26   0   94M  0 part
│ └─vg1-lv1        253:2    0  184M  0 lvm   /my-lvm      ....... nicely mounted
├─sdb11              8:27   0   95M  0 part
│ └─vg1-lv1        253:2    0  184M  0 lvm   /my-lvm
├─sdb12              8:28   0   94M  0 part



[root@server1 ~]# mount  | grep vg1-lv1
/dev/mapper/vg1-lv1 on /my-lvm type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)








file or directory to be created
--------------------------------------
+----------------------+
|                      |
|   Create Directory   |
|   mkdir /my-lvm/dir1 |
|                      |
+----------------------+
             |
             v
+----------------------+
|                      |
|   Create Files       |
|   touch /my-lvm/file1|
|   touch /my-lvm/file2|
|                      |
+----------------------+
             |
             v
+----------------------+
|                      |
|   List Contents      |
|   ls -l /my-lvm/     |
|                      |
+----------------------+

                            [root@server1 ~]# mkdir /my-lvm/dir1
                            
                            [root@server1 ~]# touch  /my-lvm/file1 file2
                            
                            [root@server1 ~]# ls -l /my-lvm/
                            total 0
                            drwxr-xr-x. 2 root root 6 Feb 26 01:39 dir1
                            -rw-r--r--. 1 root root 0 Feb 26 01:39 file1




copying file to lvm space 
-------------------------------------
[root@server1 ~]# find /usr/share/doc/ -name '*.pdf' -exec cp {} /my-lvm/ \;

[root@server1 ~]# ls /my-lvm/
dir1  file1  pigz.pdf





checking size of space
-----------------------
[root@server1 ~]# df -h | grep my-lvm
Filesystem                  Size  Used Avail Use% Mounted on
/dev/mapper/vg1-lv1         179M   11M  168M   6% /my-lvm    ....... 11M used















06_03-Resizing Logical Volumes on the Fly
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


[root@server1 ~]# df -h | grep my-lvm
Filesystem                  Size  Used Avail Use% Mounted on
/dev/mapper/vg1-lv1         179M   11M  168M   6% /my-lvm  ..........6% or  168M avilable



need more space?  use vgextend
-----------------------

[root@server1 ~]# pvscan
  PV /dev/sdb10   VG vg1             lvm2 [92.00 MiB / 0    free]
  PV /dev/sdb11   VG vg1             lvm2 [92.00 MiB / 0    free]
  PV /dev/sda2    VG cs_master       lvm2 [<29.00 GiB / 0    free]
  PV /dev/sdb12                      lvm2 [94.00 MiB]        .......................... check physical volume that is not been used,
  Total: 4 [<29.27 GiB] / in use: 3 [<29.18 GiB] / in no VG: 1 [94.           00 MiB]










[root@server1 ~]# vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  cs_master   1   2   0 wz--n- <29.00g    0
  vg1         2   1   0 wz--n- 184.00m    0        vg1 has size 184M

[root@server1 ~]# vgscan
  Found volume group "vg1" using metadata type lvm2
  Found volume group "cs_master" using metadata type lvm2


[root@server1 ~]# vgextend vg1 /dev/sdb12      
  Volume group "vg1" successfully extended    ............. old vg1(184) + 
 new sdb12(94M) = 278 M is extended on vg1

[root@server1 ~]# vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  cs_master   1   2   0 wz--n- <29.00g     0
  vg1         3   1   0 wz--n- 276.00m 92.00m


[root@server1 ~]# vgscan
  Found volume group "vg1" using metadata type lvm2
  Found volume group "cs_master" using metadata type lvm2




now extend on LV from VG
----------------------------

[root@server1 ~]# lvs
  LV   VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root cs_master -wi-ao---- <26.00g                                           
  swap cs_master -wi-ao----   3.00g                                           
  lv1  vg1       -wi-ao---- 184.00m       .................. old lv size = 184M




[root@server1 ~]# lvextend -L +50m /dev/vg1/lv1      ...............+50 m added from vg1 
  Rounding size to boundary between physical extents: 52.00 MiB.
  Size of logical volume vg1/lv1 changed from 184.00 MiB (46 extents) to 236.00 MiB (59 extents).
  Logical volume vg1/lv1 successfully resized.



[root@server1 ~]# lvs
  LV   VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root cs_master -wi-ao---- <26.00g                                           
  swap cs_master -wi-ao----   3.00g                                           
  lv1  vg1       -wi-ao---- 236.00m    ................. new lv1 size 184 to 236 m





[root@server1 ~]# df -h /my-lvm/
Filesystem           Size  Used Avail Use% Mounted on
/dev/mapper/vg1-lv1  179M   11M  168M   6% /my-lvm  ............yuck file system is same ..... need to resize



[root@server1 ~]# xfs_growfs /my-lvm/            ........... resizing file system


[root@server1 ~]# df -h /my-lvm/
Filesystem           Size  Used Avail Use% Mounted on
/dev/mapper/vg1-lv1  231M   12M  220M   5% /my-lvm  ...........walla not storage grows from 168 to 220M











and the file remians persistence, not crashed
-----------------------------------------------
[root@server1 ~]# ls -l /my-lvm/
total 12
drwxr-xr-x. 2 root root     6 Feb 26 01:39 dir1
-rw-r--r--. 1 root root     0 Feb 26 01:39 file1
-rw-r--r--. 1 root root 10732 Feb 26 01:54 pigz.pdf





















06_04-LVM Snapshots
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


LVM snapshots are a feature of the Logical Volume Manager (LVM) that 
allows you to create a read-only, point-in-time copy (snapshot) of a 
logical volume. 

Snapshots are useful for various purposes, such as backup operations 
or creating a consistent state for data analysis.



Here's an overview of LVM snapshots:
---------------------------------------


    Snapshot Volume:
    ---------------------------
        A snapshot volume is a temporary copy of 
        a logical volume. 
        It provides a consistent view of the data at
        the time the snapshot is created.
    
    
    Read-Only:
    ---------------------------
        Snapshots are read-only, meaning you can access the
        data from the snapshot, but you cannot modify it.
    
    
    Copy-on-Write (COW):
    ---------------------------
        LVM snapshots use a copy-on-write mechanism. 
        Initially, the snapshot volume contains no data, and
        changes made to the original volume are stored in the snapshot.



Creating an LVM Snapshot:
---------------------------

    Create a Snapshot:
    ---------------------
          lvcreate --snapshot --size 1G --name snapshot1 /dev/myvg/mylv
    
        This command creates a snapshot named "snapshot1" 
        of the logical volume "/dev/myvg/mylv" with a 
        size of 1 gigabyte.
    
    
    Mount the Snapshot:
    ---------------------
          mount /dev/myvg/snapshot1 /mnt/snapshot1
    
        This command mounts the snapshot volume to a 
        mount point for examination.

    
    Access Data:
    ---------------------
        You can now access the data from the 
        snapshot at /mnt/snapshot1 without affecting 
        the original volume.
    
    
    Remove the Snapshot:
    ---------------------
    
          lvremove /dev/myvg/snapshot1
    
        After you are done with the snapshot, you
        can remove it to free up space.




Use Cases:
---------------------
        Backup:
        --------
            Creating a snapshot before performing backups ensures
            a consistent state of the data.
        
        
        Testing:
        --------
            Snapshots can be used for testing changes without 
            modifying the original data.
        
        
        Data Recovery:
        --------
            In case of accidental data modification, a snapshot
            provides a point-in-time recovery option.





Important Considerations:
----------------------------
      Storage Space:
      ----------------
          The size of the snapshot should be carefully
          chosen based on the changes expected in the original 
          volume during its existence.
      
      
      Performance Impact:
      ----------------
          There is a performance impact when creating and 
          maintaining snapshots, as changes are tracked using 
          the COW mechanism.
      
      
      Time Limit:
      ----------------
          Snapshots should not be kept for an extended 
          period to avoid running out of space.



  Always refer to the official documentation or specific guides for 
  your Linux distribution for detailed and accurate instructions on 
  working with LVM snapshots.
  


[root@server1 ~]# vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  cs_master   1   2   0 wz--n- <29.00g     0
  vg1         3   1   0 wz--n- 276.00m 40.00m    ............40m free space for snapshot taking 





taking snapshot (-s) of 30m from 40m name backup
----------------------------------------------------
[root@server1 ~]# lvcreate -L 30m -s -n backup /dev/vg1/lv1
  Rounding up size to full physical extent 32.00 MiB
  Logical volume "backup" created.    ........................ no need to format



[root@server1 ~]# mount /dev/vg1/backup /mnt/ -o nouuid,ro

[root@server1 ~]# ls /mnt/
dir1  file1  pigz.pdf                      ..................... yes the files can be viewed, chek as snapshot







snapshot checking on deleting 
--------------------------------
[root@server1 ~]# ls /mnt/
dir1  file1  pigz.pdf

[root@server1 ~]# ls /my-lvm/
dir1  file1  pigz.pdf

[root@server1 ~]# rm -rf /my-lvm/file1    ............ file deleted from lv

[root@server1 ~]# ls /my-lvm/        ............. no file foid in lv
dir1  pigz.pdf

[root@server1 ~]# ls /mnt/    ......................... but snap shot preserves file1 as backup
dir1  file1  pigz.pdf







[root@server1 ~]# tar -cf /root/backup.tar /mnt/

[root@server1 ~]# ls -lh  /root/backup.tar
-rw-r--r--. 1 root root 20K Feb 26 02:34 /root/backup.tar    .......... the backup file


[root@server1 ~]# umount /mnt


[root@server1 ~]# lvremove /dev/vg1/backup
Do you really want to remove active logical volume vg1/backup? [y/n]: y
  Logical volume "backup" successfully removed.














06_05-Migrating PVs to New Storage
+++++++++++++++++++++++++++++++++++++++++++++++++++++++  SDC


[root@server1 ~]# poweroff


add disk on server1 == 8G from vm(VMWARE)
---------------------------------------
          stop server1 ==> settigs ==> Hard Disk ==> 
                                      add ==> SCSI 
                                          ==> create a virtual disk
                                          ==> max size of disk = 8G
                                          ==> disk file = lfcs3-server2.vmdk
          
              START MACHINE



+---------------------------+
|                           |
|   Migrate PVs to New      |
|   Storage                 |
|                           |
+---------------------------+
             |
             v
+---------------------------+
|                           |
|   Create PVs on New       |
|   Storage                 |
|   pvcreate /dev/new_sdc1  |
|   pvcreate /dev/new_sdc2  |
|                           |
+---------------------------+
             |
             v
+---------------------------+
|                           |
|   Extend VG with New PVs  |
|   vgextend vg1 /dev/new_sdc1|
|   vgextend vg1 /dev/new_sdc2|
|                           |
+---------------------------+
             |
             v
+---------------------------+
|                           |
|   Migrate LVs to New PVs  |
|   pvmove /dev/old_sdb1    |
|   pvmove /dev/old_sdb2    |
|                           |
+---------------------------+
             |
             v
+---------------------------+
|                           |
|   Remove Old PVs          |
|   vgreduce vg1 /dev/old_sdb1|
|   vgreduce vg1 /dev/old_sdb2|
|                           |
+---------------------------+






[root@server1 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part  /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm   /
  └─cs_master-swap 253:1    0    3G  0 lvm   [SWAP]
sdb                  8:16   0    8G  0 disk
├─sdb1               8:17   0    1K  0 part
├─sdb5               8:21   0   93M  0 part  [SWAP]
├─sdb6               8:22   0   95M  0 part  /data/mydata
├─sdb7               8:23   0   94M  0 part  /data/mydata2
├─sdb8               8:24   0   94M  0 part
├─sdb9               8:25   0   95M  0 part
├─sdb10              8:26   0   94M  0 part
│ └─vg1-lv1        253:2    0  236M  0 lvm   /my-lvm
├─sdb11              8:27   0   95M  0 part
│ └─vg1-lv1        253:2    0  236M  0 lvm   /my-lvm
├─sdb12              8:28   0   94M  0 part
│ └─vg1-lv1        253:2    0  236M  0 lvm   /my-lvm
├─sdb13              8:29   0   94M  0 part
│ └─md0              9:0    0   93M  0 raid1
└─sdb14              8:30   0   95M  0 part
  └─md0              9:0    0   93M  0 raid1
sdc                  8:32   0    8G  0 disk  .................... new disk of 8k added
sr0                 11:0    1 1024M  0 rom




[root@server1 ~]# fdisk /dev/sdc

Command (m for help): n    *******************
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p):

Using default response p. 
Partition number (1-4, default 1): 
First sector (2048-16777215, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-16777215, default 16777215): +300M  *******************

Created a new partition 1 of type 'Linux' and of size 300 MiB.

Command (m for help): t
Selected partition 1
Hex code (type L to list all codes): 8e   *******************
Changed type of partition 'Linux' to 'Linux LVM'.

Command (m for help): w   *******************
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.










[root@server1 ~]# fdisk -l /dev/sdc
Device     Boot Start    End Sectors  Size Id Type
/dev/sdc1        2048 616447  614400  300M 8e Linux LVM











[root@server1 ~]# lsblk  /dev/sdc
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdc      8:32   0    8G  0 disk
└─sdc1   8:33   0  300M  0 part




[root@server1 ~]# pvcreate /dev/sdc1
  Physical volume "/dev/sdc1" successfully created.


[root@server1 ~]# vgextend vg1 /dev/sdc1
  Volume group "vg1" successfully extended    .............. adding in vg1 group



[root@server1 ~]# pvmove /dev/sdb10  /dev/sdc1
  /dev/sdb10: Moved: 100.00%





[root@server1 ~]# vgreduce vg1 /dev/sdb10
  Removed "/dev/sdb10" from volume group "vg1"


[root@server1 ~]# pvremove /dev/sdb10    ...........remove header info
  Labels on physical volume "/dev/sdb10" successfully wiped.




[root@server1 ~]# vgs vg1
  VG  #PV #LV #SN Attr   VSize   VFree
  vg1   3   1   0 wz--n- 480.00m 244.00m  ............. 244 free for next lv




[root@server1 ~]# ls /my-lvm/  ............ viewing persistence of files
dir1  pigz.pdf
[root@server1 ~]# file /my-lvm/
/my-lvm/: directory
[root@server1 ~]# file /my-lvm/pigz.pdf
/my-lvm/pigz.pdf: PDF document, version 1.4




[root@server1 ~]# umount /my-lvm
[root@server1 ~]# mount -a
[root@server1 ~]# ls /my-lvm/
dir1  pigz.pdf    ...........................data persist after umount
























===============================================================================
07. Configuring an iSCSI Block Storage Server
===============================================================================

                          +--------------------------+
                          |                          |
                          |   iSCSI Initiator        |
                          |                          |
                          +--------------------------+
                                       |
                                       v
                          +--------------------------+
                          |                          |
                          |    TCP/IP Network        |
                          |    (Ethernet/fiber)      |
                          |                          |
                          +--------------------------+
                                       |
                                       v
                          +--------------------------+
                          |                          |
                          |   iSCSI Target           |
                          |                          |
                          +--------------------------+


    iSCSI (Internet Small Computer System Interface) 
    Block Storage  is a protocol used for accessing and managing 
    storage devices over a TCP/IP network. 

    iSCSI share block , often LVM's
    
    It enables the transfer of block-level data between an 
    iSCSI initiator (client) and an iSCSI target (storage device). 
    
    iSCSI provides a flexible and cost-effective way to implement 
    storage area networks (SANs) over existing Ethernet networks.



Here are key features and components of iSCSI Block Storage:
----------------------------------------------------------
      Block-Level Storage:
      -------------------------------
          iSCSI operates at the block level, allowing the 
          transfer of raw storage blocks between the initiator (client) 
          and target (storage device).
          This contrasts with file-level protocols like NFS or SMB.
      
      
      Initiator and Target:
      -------------------------------
          Initiator: 
          ---------
          The iSCSI initiator is the client device that 
          initiates communication with the iSCSI target. 
          It can be a server, workstation, or any device requiring 
          access to block storage.
          
          Target: 
          ---------
          The iSCSI target is the storage device or server 
          that provides access to the block-level storage. 
          It can be a dedicated storage appliance or a server with 
          iSCSI target software.
      
      
      TCP/IP Network:
      -------------------------------
          iSCSI leverages the standard TCP/IP network infrastructure 
          for communication. 
          This makes it easy to integrate with existing Ethernet 
          networks, eliminating the need for specialized 
          Fibre Channel networks.
      
      
      Logical Unit Number (LUN):
      -------------------------------
          A Logical Unit Number (LUN) represents a logical 
          addressable storage unit provided by the iSCSI target. 
          Each LUN is associated with specific storage capacity 
          on the target.
      
      
      iSCSI Initiator Software:
      -------------------------------
          Initiator software is required on the client side to
          enable communication with the iSCSI target. 
          Most modern operating systems come with built-in iSCSI
          initiator software, making it relatively easy to configure.
      
      
      
      Security:
      -------------------------------
          iSCSI supports security features such as
          CHAP (Challenge-Handshake Authentication Protocol)
          for authentication and IPsec for data encryption. 
          These features help secure the communication 
          between the initiator and target.
      
      
      Use Cases:
      -------------------------------
          iSCSI Block Storage is commonly used in scenarios 
          where block-level access to storage is required, 
          such as virtualization environments, database storage, 
          and shared storage for clustered servers.
      
      
      Storage Virtualization:
      -------------------------------
          iSCSI enables storage virtualization by presenting 
          remote storage as local block devices to the initiators. 
          This allows for centralized storage management and
          resource utilization.
      
      
      Performance:
      -------------------------------
          While iSCSI operates over standard Ethernet networks, dedicated 
          storage networks with higher bandwidth and low latency (such 
          as 10GbE or faster) are often recommended for optimal performance.
          


iSCSI Block Storage provides a versatile and cost-effective 
solution for organizations seeking centralized and scalable 
storage solutions. 

It is commonly used in conjunction with storage technologies 
like RAID (Redundant Array of Independent Disks) for data 
protection and performance enhancement.






07_01-iSCSI and SANS
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




07_02-Install iSCSI Target and Configure Firewall
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
       kernel driven software


[root@server1 ~]# yum install targetd targetcli -y


[root@server1 ~]# systemctl enable targetd.service
Created symlink /etc/systemd/system/multi-user.target.wants/targetd.service → /usr/lib/systemd/system/targetd.service.




[root@server1 csf]# firewall-cmd --permanent --add-service=iscsi-target
success
[root@server1 csf]# firewall-cmd --reload
success









07_03-Create Logical Volume to Share as Block Device
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# vgs vg1
  VG  #PV #LV #SN Attr   VSize   VFree
  vg1   3   1   0 wz--n- 480.00m 244.00m    ............244m free



[root@server1 ~]# lvcreate -L 100m -n web_lv vg1
  Logical volume "web_lv" created.    ...... NOTE sharing a block storage , not file system (xfs, ext4)





[root@server1 ~]# lvscan
  ACTIVE            '/dev/vg1/lv1' [236.00 MiB] inherit
  ACTIVE            '/dev/vg1/web_lv' [100.00 MiB] inherit ................
  ACTIVE            '/dev/cs_master/swap' [3.00 GiB] inherit
  ACTIVE            '/dev/cs_master/root' [<26.00 GiB] inherit



  LV     VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cs_master -wi-ao---- <26.00g                               
  swap   cs_master -wi-ao----   3.00g                               
  lv1    vg1       -wi-ao---- 236.00m                               
  web_lv vg1       -wi-a----- 100.00m  .......................









07_04-Configure iSCSI Target
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


[root@server1 ~]# hostname
server1.sanjeevthapa.com





[root@server1 ~]# targetcli                 .....................


Warning: Could not load preferences file /root/.targetcli/prefs.bin.
targetcli shell version 2.1.53
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/> ls
o- / ......................................................... [...]
  o- backstores .............................................. [...]
  | o- block .................................. [Storage Objects: 0]
  | o- fileio ................................. [Storage Objects: 0]
  | o- pscsi .................................. [Storage Objects: 0]
  | o- ramdisk ................................ [Storage Objects: 0]
  o- iscsi ............................................ [Targets: 0]
  o- loopback ......................................... [Targets: 0]
/>


/> backstores/block create web_store /dev/vg1/web_lv      .....................
Created block storage object web_store using /dev/vg1/web_lv.
/> ls
o- / ......................................................... [...]
  o- backstores .............................................. [...]
  | o- block .................................. [Storage Objects: 1]
  | | o- web_store  [/dev/vg1/web_lv (100.0MiB) write-thru deactivated]




/> iscsi/ create iqn.2024-02.com.sanjeevthapa.server1:web          .....................
Created target iqn.2024-02.com.sanjeevthapa.server1:web.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.
/> ls

  o- iscsi ............................................ [Targets: 1]
  | o- iqn.2024-02.com.sanjeevthapa.server1:web .......... [TPGs: 1]
  |   o- tpg1 ............................... [no-gen-acls, no-auth]
  |     o- acls .......................................... [ACLs: 0]
  |     o- luns .......................................... [LUNs: 0]
  |     o- portals .................................... [Portals: 1]
  |       o- 0.0.0.0:3260 ..................................... [OK}  ........ listening port 3260


/> cd iscsi/iqn.2024-02.com.sanjeevthapa.server1:web/tpg1/luns    ........ listening port 3260
/iscsi/iqn.20...web/tpg1/luns> create /backstores/block/web_store    ........ 
Created LUN 0.






/iscsi/iqn.20...a.server1:web> cd tpg1/acls      ...............
/iscsi/iqn.20...web/tpg1/acls> create iqn.2024-02.com.mivaanthapa.server2:web    ............... client acesss ACL ...
Created Node ACL for iqn.2024-02.com.mivaanthapa.server2:web
Created mapped LUN 0.









/iscsi/iqn.20...web/tpg1/acls> cd /

/> ls
o- / ................................................................... [...]
  o- backstores ........................................................ [...]
  | o- block ............................................ [Storage Objects: 1]
  | | o- web_store ......... [/dev/vg1/web_lv (100.0MiB) write-thru activated]
  | |   o- alua ............................................. [ALUA Groups: 1]
  | |     o- default_tg_pt_gp ................. [ALUA state: Active/optimized]
  | o- fileio ........................................... [Storage Objects: 0]
  | o- pscsi ............................................ [Storage Objects: 0]
  | o- ramdisk .......................................... [Storage Objects: 0]
  o- iscsi ...................................................... [Targets: 1]
  | o- iqn.2024-02.com.sanjeevthapa.server1:web .................... [TPGs: 1]
  |   o- tpg1 ......................................... [no-gen-acls, no-auth]
  |     o- acls .................................................... [ACLs: 1]
  |     | o- iqn.2024-02.com.mivaanthapa.server2:web ........ [Mapped LUNs: 1]
  |     |   o- mapped_lun0 ....................... [lun0 block/web_store (rw)]
  |     o- luns .................................................... [LUNs: 1]
  |     | o- lun0 ..... [block/web_store (/dev/vg1/web_lv) (default_tg_pt_gp)]
  |     o- portals .............................................. [Portals: 1]
  |       o- 0.0.0.0:3260 ............................................... [OK]
  o- loopback ................................................... [Targets: 0]
/>


/> exit
Global pref auto_save_on_exit=true
Configuration saved to /etc/target/saveconfig.json





[root@server1 ~]# netstat -tnl | grep 3260
tcp        0      0 0.0.0.0:3260            0.0.0.0:*               LISTEN 








07_05-Configure iSCSI Initiator(Client)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++ SERVER2


server1.sanjeevthapa.com          .............iSCSI Target / must be running 
[root@server1 ~]# hostname -I
192.168.1.111


[root@server2 ~]# hostname    ...............iSCSI Initiator
server2.mivaanthapa.com
[root@server2 ~]# hostname -I
192.168.1.112




[root@server2 ~]# yum list available  | grep iscsi
iscsi-initiator-utils.i686                             6.2.1.4-8.git095f59c.el8  




[root@server2 ~]# yum  -y install iscsi-initiator-utils



[root@server2 ~]# vi /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2024-02.com.mivaanthapa.server2:web                  ...... server2 iqn




[root@server2 ~]# iscsiadm --mode discovery --type sendtargets --portal  192.168.1.111 --discover                                      .................. walla it finds iscsi target
192.168.1.111:3260,1 iqn.2024-02.com.sanjeevthapa.server1:web


                                            OR

iscsiadm --mode discovery --type sendtargets --portal  server1.sanjeevthapa.com --discover      .............................. make entry ip and domain in ... hosts file










connecting isci target 
------------------------ sharing bock device (by isci)

[root@server2 ~]# lsblk
NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                  8:0    0   30G  0 disk
├─sda1               8:1    0    1G  0 part /boot
└─sda2               8:2    0   29G  0 part
  ├─cs_master-root 253:0    0   26G  0 lvm  /
  └─cs_master-swap 253:1    0    3G  0 lvm  [SWAP]
sr0                 11:0    1 1024M  0 rom



connecting isci target 
------------------------
[root@server2 ~]# iscsiadm --mode node --targetname iqn.2024-02.com.sanjeevthapa.server1:web  --portal 192.168.1.111 --login

Logging in to [iface: default, target: iqn.2024-02.com.sanjeevthapa.server1:web, portal: 192.168.1.111,3260]
Login to [iface: default, target: iqn.2024-02.com.sanjeevthapa.server1:web, portal: 192.168.1.111,3260] successful.





[root@server2 ~]# lsblk  /dev/sdb
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdb    8:16   0  100M  0 disk  ................................ NOW 100M of block storage is attached from server1




AFTER THAT MAKE YOURS FILE SYSTEM ................ MOUNT IT ............... CREATE A FILE ........ N CHECK IT
























===============================================================================
08. Implementing HA Clusters
===============================================================================

+----------------------+
|                      |
|  HA Cluster Scenario |
|                      |
+----------------------+
             |
             v
+----------------------+
|                      |
|   Node 1 (Active)    |
|   [Service A]        |
|   [Service B]        |
|   [Virtual IP]        |
|                      |
+----------------------+
             |
             v
+----------------------+
|                      |
|   Node 2 (Passive)   |
|   [Service A]        |
|   [Service B]        |
|                      |
+----------------------+


Implementing High Availability (HA) clusters involves setting 
up a redundant and fault-tolerant infrastructure to ensure 
continuous availability of services in the event of 
hardware failures, software issues, or other disruptions. 




Here is a general outline of the steps involved in implementing HA clusters:
---------------------------------------------------------------------------

1. Define Requirements and Scope:
--------------------------------------------
        Identify critical services and applications that need high availability.
      
        Determine the level of redundancy and fault tolerance required.
      
        Define the scope of the HA cluster, considering both hardware 
        and software components.


2. Choose HA Cluster Software:
--------------------------------------------
      Select an appropriate HA clustering solution based on your 
      platform and requirements. 
      
      Common choices include:
              Linux-HA (Corosync/Pacemaker): 
                        Widely used for Linux environments.
              
              Microsoft Failover Cluster: 
                        For Windows Server environments.
      
              Veritas Cluster Server (VCS): 
                        Provides cross-platform support.




3. Design Cluster Architecture:
--------------------------------------------
      Plan the architecture with considerations for redundancy, failover 
      mechanisms, and load balancing.
      
      Define the number of nodes in the cluster and their 
      roles (active, passive, or both).
      
      Configure networking settings and IP addresses for virtual IPs (VIPs).




4. Hardware Redundancy:
--------------------------------------------
      Ensure hardware redundancy for critical components such as 
      power supplies, network interfaces, and storage.
      
      Use multiple servers to eliminate single points of failure.




5. Shared Storage:
--------------------------------------------
      If required, set up shared storage to be accessible by 
      all nodes in the cluster.
      
      Consider redundant storage options like SAN (Storage Area Network) 
      or distributed file systems.



6. Configure Cluster Software:
--------------------------------------------
      Install and configure the chosen HA cluster software on each node.
      
      Define cluster resources, services, and dependencies.
      
      Configure fencing mechanisms to isolate failed nodes.




7. Test Failover Scenarios:
--------------------------------------------
      Conduct thorough testing of failover scenarios to ensure that 
      the cluster behaves as expected during various failure conditions.
      
      Test node failures, network failures, and application/service failovers.




8. Monitor and Manage:
--------------------------------------------
      Implement monitoring tools to continuously monitor the health 
      of the cluster and its components.
      
      Set up alerts and notifications for potential issues.
      
      Implement a management interface for manual intervention 
      and configuration changes.


9. Documentation:
--------------------------------------------
      Create detailed documentation for the HA cluster setup, 
      configuration, and maintenance procedures.
      
      Include information on troubleshooting and recovery procedures.




10. Maintenance and Updates:
--------------------------------------------
      Regularly update and patch the HA cluster software a
      nd operating system.
      
      Plan for scheduled maintenance windows to avoid 
      service disruptions.



11. Training and Documentation:
--------------------------------------------
      Provide training to administrators on managing and 
      troubleshooting the HA cluster.
      
      Keep documentation up-to-date as the cluster evolves.



12. Disaster Recovery Plan:
--------------------------------------------
      Develop a disaster recovery plan outlining procedures for
      recovering from catastrophic failures.





  Remember that the specific steps and considerations may vary 
  depending on the chosen HA clustering solution and the operating 
  systems involved. 
  
  Always refer to the documentation provided by the HA clustering 
  software for detailed instructions and best practices.








08_01-Introduction to HA Clusters
+++++++++++++++++++++++++++++++++++++++++++++++++++++++
                   +----------------------+
                   |    Load Balancer     |
                   |   Virtual IP:        |
                   |   192.168.1.110      |
                   +----------+-----------+
                              |
            +-----------------|---------------------+
            |                                       |
+-----------v----------------+         +------------v------------+
|  Server1                   |         |  Server2                |
|                            |         |                         |
|  server1.sanjeevthapa.com  |         | server2.mivaanthapa.com |
|  192.168.1.111             |         | 192.168.1.112           |
+----------------------------+         +-------------------------+







08_02-Installing Pacemaker
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# vi /etc/yum.repos.d/CentOS-Stream-HighAvailability.repo    ...... in server2 also
    enabled=1
:wq

[root@server1 ~]# yum repolist all | grep ha
ha                             CentOS Stream 8 - HighAvailability       enabled


[root@server1 ~]# yum install pacemaker pcs resource-agents

[root@server2 ~]# yum install pacemaker pcs resource-agents

[root@server1 ~]# cat /etc/passwd | grep hacluster
hacluster:x:189:189:cluster user:/home/hacluster:/sbin/nologin



[root@server1 ~]# echo 'hacluster:Password' | chpasswd

[root@server2 ~]#  echo 'hacluster:Password' | chpasswd





[root@server1 ~]# firewall-cmd --permanent --add-service=high-availability
success
[root@server1 ~]# firewall-cmd --reload
success



[root@server2 ~]# firewall-cmd --permanent --add-service=high-availability
success
[root@server2 ~]# firewall-cmd --reload
success








[root@server1 ~]# echo 'Password' | passwd --stdin hacluster
Changing password for user hacluster.
passwd: all authentication tokens updated successfully.



[root@server2 ~]# echo 'Password' | passwd --stdin hacluster
Changing password for user hacluster.
passwd: all authentication tokens updated successfully.











08_03-Creating/ configure  the Cluster
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# systemctl enable pcsd
.
[root@server1 ~]# systemctl start pcsd





[root@server2 ~]#  systemctl enable pcsd

[root@server2 ~]#  systemctl start pcsd




















08_04-Understanding STONITH and QUORUM
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




08_05-Clustering an IP Address
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




08_06-Installing and Configuring Apache
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




08_07-Clustering Apache
+++++++++++++++++++++++++++++++++++++++++++++++++++++++








===============================================================================
09. Implementing Aggregated Storage with GlusterFS
===============================================================================






09_01-Welcome to the GlusterFS
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




09_02-Adding and Preparing Disks
+++++++++++++++++++++++++++++++++++++++++++++++++++++++




+++++++++++++++++++++++++++++++++++++++++++++++++++++++





===============================================================================
10. Encrypted Volumes
===============================================================================


    Encrypted volumes refer to storage spaces or partitions on a 
    computer or server that have an added layer of security through encryption. 
    
    Encryption is the process of converting data into a format that 
    is unreadable without the appropriate decryption key or password. 
    
    Encrypted volumes protect the data stored on them from unauthorized 
    access, ensuring confidentiality and privacy.


    The encryption process involves transforming the plaintext data
    into ciphertext using an encryption algorithm and a secret key.
    
    The encrypted data is then stored on the volume. To access the 
    data, a user must provide the correct decryption key or password.

    One common method for creating encrypted volumes on Linux systems
    is to use the Linux Unified Key Setup (LUKS) standard along with 
    tools like cryptsetup. 
    
    LUKS provides a platform-independent on-disk format for 
    encrypted volumes, ensuring compatibility across various Linux distributions.





Benefits of using encrypted volumes include:
-----------------------------------------------
      Data Security: 
      ----------------
          Encrypted volumes protect sensitive data from unauthorized
          access, even if physical access to the storage device is gained.


      Confidentiality:
      ----------------
          Encrypting volumes ensures that the data remains confidential, 
          preventing unauthorized users or entities from reading or modifying it.


      Compliance: 
      ----------------
          Encrypting sensitive data is often a requirement for compliance
          with various data protection regulations and standards.


      Data-at-Rest Protection: 
      -----------------------
          Encrypted volumes provide protection for data at rest, meaning the
          data is secure even when the system is powered off.


      Additional Security Layers: 
      -----------------------
          Encrypted volumes can be part of a broader security strategy,
          providing an additional layer of defense against data breaches.
      
      


Common use cases for encrypted volumes include securing sensitive 
files, protecting personal or financial information, and ensuring 
the confidentiality of data stored on servers or storage devices. 

It's important to manage encryption keys or passwords securely, as 
they are critical for accessing the encrypted data.




10_01-Full Disk Encryption
+++++++++++++++++++++++++++++++++++++++++++++++++++++++












10_02-Shredding Disks
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# vgs
  VG        #PV #LV #SN Attr   VSize   VFree
  cs_master   1   2   0 wz--n- <29.00g      0
  vg1         3   2   0 wz--n- 480.00m 144.00m    .............144 aviable




[root@server1 ~]# lvcreate -L 60m -n enc_disk vg1
  Logical volume "enc_disk" created.




[root@server1 ~]# shred -v --iterations=1 /dev/vg1/enc_disk
shred: /dev/vg1/enc_disk: pass 1/1 (random)...      ...............encrepted volume






10_03-Encrypting Disks
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



[root@server1 ~]# grep -i ACL /boot/config-$(uname -r)
CONFIG_EXT4_FS_POSIX_ACL=y
CONFIG_XFS_POSIX_ACL=y
CONFIG_FS_POSIX_ACL=y
CONFIG_TMPFS_POSIX_ACL=y
CONFIG_NFS_V3_ACL=y
CONFIG_NFSD_V2_ACL=y
CONFIG_NFSD_V3_ACL=y
CONFIG_NFS_ACL_SUPPORT=m
CONFIG_CEPH_FS_POSIX_ACL=y




          kernel configuration options for disk encryption:
          ------------------------------------------------
                Device Mapper (DM):
                    CONFIG_BLK_DEV_DM: Device mapper support
                    CONFIG_DM_VERITY: Device mapper support for verity target
                    CONFIG_DM_SWITCH: Device mapper support for switch target
                    CONFIG_DM_LOG_WRITES: Device mapper support for log writes target
                    CONFIG_DM_SNAPSHOT: Device mapper support for snapshot target
                    CONFIG_DM_THIN_PROVISIONING: Device mapper support for thin provisioning target
                    CONFIG_DM_ZERO: Device mapper support for zero target
                    ...
              
          
              Cryptographic API:
                    CONFIG_CRYPTO: Cryptographic API
                    CONFIG_CRYPTO_XTS: XTS block cipher mode
                    CONFIG_CRYPTO_CBC: CBC block cipher mode
                    ...
          
          
              File System Encryption:
                  CONFIG_DM_CRYPT: Device mapper support for transparent data encryption
                  CONFIG_FS_ENCRYPTION: Filesystem Encryption (EXPERIMENTAL)
                  CONFIG_FSCRYPT: Filesystem crypto API support
                  ...
          
              LUKS (Linux Unified Key Setup):
                    CONFIG_DM_CRYPT: Device mapper support for transparent data encryption
                    CONFIG_CRYPTO_AES: AES cipher algorithms
                    CONFIG_CRYPTO_XTS: XTS block cipher mode
                    ...
          
          
          
[root@server1 ~]# grep -i DM_CRYPT /boot/config-$(uname -r)
CONFIG_DM_CRYPT=m            ............may be support by kernel modeule



[root@server1 ~]# lsmod | grep dm_crypt
[root@server1 ~]#



[root@server1 ~]# modprobe dm_crypt        .........shelf load
[root@server1 ~]# lsmod | grep dm_crypt
dm_crypt               49152  0
dm_mod                155648  20 dm_crypt,dm_log,dm_mirror





[root@server1 ~]# rpm -qf $(which cryptsetup)
cryptsetup-2.3.7-7.el8.x86_64





[root@server1 ~]# cryptsetup -y luksFormat /dev/vg1/enc_disk

WARNING!
========
This will overwrite data on /dev/vg1/enc_disk irrevocably.

Are you sure? (Type 'yes' in capital letters): YES
Enter passphrase for /dev/vg1/enc_disk: A@pple@1234
Verify passphrase: A@pple@1234



[root@server1 ~]# echo $?
0





[root@server1 ~]# cryptsetup luksDump /dev/vg1/enc_disk
LUKS header information
Version:        2
Epoch:          3
Metadata area:  16384 [bytes]
Keyslots area:  16744448 [bytes]
UUID:           5203bfcc-ae12-4ad5-98d7-204df3e14f7e
Label:          (no label)
Subsystem:      (no subsystem)
Flags:          (no flags)

Data segments:
  0: crypt
        offset: 16777216 [bytes]
        length: (whole device)
        cipher: aes-xts-plain64
        sector: 512 [bytes]

Keyslots:
  0: luks2
        Key:        512 bits
        Priority:   normal
        Cipher:     aes-xts-plain64
        Cipher key: 512 bits
        PBKDF:      argon2i
        Time cost:  4
        Memory:     337408
        Threads:    1
        Salt:       74 26 e7 66 3b 8e c1 f4 92 87 eb aa 35 d4 64 23
                    40 dd c6 43 71 ad cb 0d d4 2c 75 88 ec 55 12 75
        AF stripes: 4000
        AF hash:    sha256
        Area offset:32768 [bytes]
        Area length:258048 [bytes]
        Digest ID:  0
Tokens:
Digests:
  0: pbkdf2
        Hash:       sha256
        Iterations: 71624
        Salt:       f1 12 dc a6 7f 81 08 fb 16 0d 38 c3 e9 ae f5 d9
                    65 5a 43 8a c2 a9 e6 fa 66 bc ed 33 d7 6b cf ce
        Digest:     08 bd a8 6a be 77 30 b8 a3 23 c4 f8 6e 24 0b 42
                    96 44 f5 6e 14 f1 ec a3 95 9d 70 c4 43 ab 36 52









[root@server1 ~]# cryptsetup isLuks /dev/vg1/enc_disk
[root@server1 ~]# echo $?
0              .............................. TRUE is encrypted disk


[root@server1 ~]# cryptsetup isLuks /dev/vg1/web_lv
[root@server1 ~]# echo $?
1              .............................. FALSE as not encrypted disk














10_04-Opening Encrypted Disks and Formatting
+++++++++++++++++++++++++++++++++++++++++++++++++++++++


[root@server1 ~]# cryptsetup luksOpen /dev/vg1/enc_disk enc_vol
Enter passphrase for /dev/vg1/enc_disk: A@pple@1234


[root@server1 ~]# echo $?
0



[root@server1 ~]# ls /dev/mapper/enc_vol
/dev/mapper/enc_vol                            ............. symbolic link




[root@server1 ~]# mkfs.xfs /dev/mapper/enc_vol







10_05-Mounting at Boot
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# ls -l /dev/mapper/
total 0
crw-------. 1 root root 10, 236 Feb 26 02:41 control
lrwxrwxrwx. 1 root root       7 Feb 26 02:41 cs_master-root -> ../dm-0
lrwxrwxrwx. 1 root root       7 Feb 26 02:41 cs_master-swap -> ../dm-1
lrwxrwxrwx. 1 root root       7 Feb 26 22:00 enc_vol -> ../dm-5
lrwxrwxrwx. 1 root root       7 Feb 26 21:57 vg1-enc_disk -> ../dm-4
lrwxrwxrwx. 1 root root       7 Feb 26 18:24 vg1-lv1 -> ../dm-2
lrwxrwxrwx. 1 root root       7 Feb 26 19:06 vg1-web_lv -> ../dm-3



[root@server1 ~]# cryptsetup luksClose enc_vol
[root@server1 ~]# echo $?
0


[root@server1 ~]# cryptsetup luksOpen /dev/vg1/enc_disk  enc_data_vol
Enter passphrase for /dev/vg1/enc_disk:  A@pple@1234




[root@server1 ~]# \ls /dev/mapper/
control         cs_master-swap  vg1-enc_disk  vg1-web_lv
cs_master-root  enc_data_vol    vg1-lv1




[root@server1 ~]# blkid  | grep vg1-enc
/dev/mapper/vg1-enc_disk: UUID="5203bfcc-ae12-4ad5-98d7-204df3e14f7e" TYPE="crypto_LUKS"




[root@server1 ~]# vi /etc/crypttab
luks-data       UUID="5203bfcc-ae12-4ad5-98d7-204df3e14f7e"


[root@server1 ~]# vim /etc/fstab
/dev/mapper/luks-data   /luks-data      xfs     defaults        0 0


[root@server1 ~]# mkdir /luks-data


[root@server1 ~]# cryptsetup luksClose enc_data_vol

[root@server1 ~]# cryptsetup luksOpen /dev/vg1/enc_disk  luks-data
Enter passphrase for /dev/vg1/enc_disk: A@pple@1234
[root@server1 ~]#



[root@server1 ~]# mount -a


[root@server1 ~]# mount | grep luks
/dev/mapper/luks-data on /luks-data type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)






[root@server1 ~]# lsblk  /dev/sdc
NAME             MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sdc                8:32   0    8G  0 disk
└─sdc1             8:33   0  300M  0 part
  ├─vg1-lv1      253:2    0  236M  0 lvm   /my-lvm
  ├─vg1-web_lv   253:3    0  100M  0 lvm
  └─vg1-enc_disk 253:4    0   60M  0 lvm
    └─luks-data  253:5    0   44M  0 crypt /luks-data



[root@server1 ~]# cat /etc/crypttab
luks-data       UUID="5203bfcc-ae12-4ad5-98d7-204df3e14f7e"


[root@server1 ~]# tail -n 1 /etc/fstab
/dev/mapper/luks-data   /luks-data      xfs     defaults        0 0



                                  ******************
                                  REBOOT SYSTEM
                                  *****************


please enter passphrase for disk vg1_enc_disk (luks0data) on /luks-data !: A@pple@1234

login as: root
root@192.168.1.111's password:
[root@server1 ~]#
[root@server1 ~]# hostname
server1.sanjeevthapa.com
[root@server1 ~]# hostname -I
192.168.1.111





















===============================================================================
11. Using the Automounter - NFS
===============================================================================
The automounter, also known as autofs, is a system service on 
Unix-like operating systems that automatically mounts and unmounts 
filesystems as needed. 

This service helps manage network filesystems, removable media, 
and other dynamic filesystems efficiently.


sudo yum install autofs
sudo nano /etc/auto.master
                  /mount_point   /etc/auto.mount_file

sudo nano /etc/auto.mount_file
                  share_name   -fstype=nfs,rw   server:/path/to/share

sudo systemctl start autofs
cd /mount_point/share_name




11_01-Using the Default AutoFS Options
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root@server1 ~]# yum list installed
[root@server1 ~]# yum list installed | grep autofs
[root@server1 ~]# yum list avilable | grep autofs


[root@server1 ~]# yum install autofs



[root@server1 ~]# ls /etc/auto*
/etc/autofs.conf            /etc/auto.master  /etc/auto.net
/etc/autofs_ldap_auth.conf  /etc/auto.misc    /etc/auto.smb





[root@server1 ~]# vim /etc/autofs.conf

[root@server1 ~]# vim /etc/auto.master



[root@server1 ~]# ls /misc
ls: cannot access '/misc': No such file or directory

[root@server1 ~]# systemctl start autofs

[root@server1 ~]# ls /misc/
[root@server1 ~]#




[root@server1 ~]# vi /etc/auto.misc    ............... mapping dir










11_02-Automounting the Encrypted Partition
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



[root@server1 misc]# lsblk  /dev/sdc
NAME             MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sdc                8:32   0    8G  0 disk
└─sdc1             8:33   0  300M  0 part
  ├─vg1-lv1      253:2    0  236M  0 lvm   /my-lvm
  ├─vg1-web_lv   253:3    0  100M  0 lvm
  └─vg1-enc_disk 253:4    0   60M  0 lvm
    └─luks-data  253:5    0   44M  0 crypt /luks-data




[root@server1 misc]# mount | grep luks
/dev/mapper/luks-data on /luks-data type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)



umount luks folder
-----------------------
[root@server1 ~]# umount /luks-data

[root@server1 ~]# mount | grep luks

[root@server1 ~]# vim /etc/fstab
  #/dev/mapper/luks-data  /luks-data      xfs     defaults        0 0    .....disable mount of luks



[root@server1 ~]# tail -n 1 /etc/fstab
#/dev/mapper/luks-data  /luks-data      xfs     defaults        0 0



[root@server1 ~]# cryptsetup luksClose luks-data
[root@server1 ~]#




[root@server1 ~]# cat /etc/crypttab
luks-data       UUID="5203bfcc-ae12-4ad5-98d7-204df3e14f7e"


[root@server1 ~]# grep misc /etc/auto.master
/misc   /etc/auto.misc
# Note that if there are entries for /net or /misc (as







[root@server1 ~]# vim /etc/auto.misc
luks            -fstype=xfs     :/dev/mapper/luks-data


[root@server1 ~]# systemctl restart autofs.service











11_03-Quick Setup of an NFS Server
+++++++++++++++++++++++++++++++++++++++++++++++++++++++



11_04-Automount Remote Mounts
+++++++++++++++++++++++++++++++++++++++++++++++++++++++










===============================================================================
12. Implement User and Group Quotas
===============================================================================


+++++++++++++++++++++++++++++++++++++++++++++++++++++++





+++++++++++++++++++++++++++++++++++++++++++++++++++++++




+++++++++++++++++++++++++++++++++++++++++++++++++++++++




===============================================================================

===============================================================================


===============================================================================

===============================================================================






===============================================================================

===============================================================================


===============================================================================

===============================================================================






===============================================================================

===============================================================================


===============================================================================

===============================================================================













